{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (生成對抗網路)\n",
    "\n",
    "生成對抗網絡（Generative Adversarial Networks, GANs）是一類由 Ian Goodfellow 於 2014 年提出的深度學習模型。GANs 包含兩個神經網絡：**生成器（Generator）**和**判別器（Discriminator）**，它們在博弈論框架下相互競爭。以下是其運作機制的詳細說明：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **生成器網絡**：\n",
    "- **生成器的任務**：生成逼真的合成數據樣本，使其與真實數據難以區分。\n",
    "- **工作原理**：將隨機噪聲向量（潛在空間）映射到目標數據分佈。\n",
    "- **目標**：生成器希望生成的樣本能夠欺騙判別器，使其無法區分真假。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **判別器網絡**：\n",
    "- **判別器的任務**：評估數據的真實性。\n",
    "- **工作原理**：接收數據樣本，嘗試區分真實數據（來自實際數據集）與假數據（由生成器生成）。\n",
    "- **目標**：提供反饋幫助生成器改進其生成的數據質量。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **訓練過程**：\n",
    "- **同時訓練**：生成器與判別器是同時訓練的。\n",
    "- **生成器目標**：最小化判別器識別假樣本的能力（通過優化損失函數）。\n",
    "- **判別器目標**：最大化判別真實數據與假數據的準確性（通過優化損失函數）。\n",
    "- **對抗過程**：訓練過程持續，直到生成器能生成足夠逼真的數據，使判別器無法可靠區分真假數據。\n",
    "\n",
    "---\n",
    "\n",
    "### 核心特點：\n",
    "- **零和博弈框架**：生成器的收益即為判別器的損失，反之亦然。\n",
    "- **無需標註數據**：GANs 依賴於無監督學習，無需標籤數據。\n",
    "- **靈活性高**：GANs 已廣泛應用於圖像生成、風格轉換、超分辨率以及視頻生成等領域。\n",
    "\n",
    "---\n",
    "\n",
    "### 挑戰：\n",
    "1. **訓練不穩定性**：\n",
    "   - 對抗訓練可能導致不收斂或模式崩塌（生成器只生成有限類型的輸出）。\n",
    "2. **對超參數的敏感性**：\n",
    "   - 適當的超參數調整是取得良好性能的關鍵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Code for GAN\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.model(z).view(-1, 1, 28, 28)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "# Initialize models, optimizers, and loss\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for real_imgs in dataloader:  # Batch of real images\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, 100)  # Random noise\n",
    "        fake_imgs = G(z)  # Generate fake images\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        # Real loss\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(D(real_imgs), real_labels)\n",
    "        fake_loss = criterion(D(fake_imgs.detach()), fake_labels)\n",
    "        D_loss = real_loss + fake_loss\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        G_loss = criterion(D(fake_imgs), real_labels)  # Fool discriminator\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample: DCGAN (Deep Convolutional Generative Adversarial Network) MNIST \n",
    "### 說明:\n",
    "這段程式碼展示了如何使用 深度卷積生成對抗網絡 (DCGAN) 訓練模型來生成手寫數字。程式碼中的主要步驟包括資料載入、生成器與判別器的定義、訓練過程，以及結果的可視化與儲存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 環境設置 (Parameter setting)\n",
    "- 裝置檢測：\n",
    "  - 程式會檢查系統是否支援 GPU（例如 CUDA 或 Apple 的 MPS）。若無 GPU，則退回使用 CPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"../data/mnist\"\n",
    "BATCH_SIZE = 64\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS backend is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS backend is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MNIST 資料載入 (Load data and transform to Dataset )\n",
    "- 使用 torchvision.datasets.MNIST 載入 MNIST 資料集。\n",
    "- 資料轉換：\n",
    "  - 將圖像縮放至 $28\\times 28$\n",
    "  - 將像素值歸一化至 [−1,1]，以符合生成器輸出的範圍（Tanh 激活函數的輸出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:13<00:00, 757kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 141kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:05<00:00, 307kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.54MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/mnist\\MNIST\\raw\n",
      "\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Load MNIST dataset\n",
    "dataset = MNIST(DATASET_PATH, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 基本參數設定及神經網路權重初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "ngf = 64  # Size of feature maps in generator\n",
    "ndf = 64  # Size of feature maps in discriminator\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Networks\n",
    "- Generator: A neural network that takes random noise ($z$) as input and outputs an image of size $28\\times28$.\n",
    "- Discriminator: A neural network that takes an image as input and outputs a probability value ($P(real)$) indicating whether the image is real (from the dataset) or fake (from the generator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 生成器 (Generator)\n",
    "- 功能：將隨機噪聲向量 $z$ 轉換為 $28\\times 28$ 的手寫數字圖像。\n",
    "- 架構：\n",
    "  - 使用反卷積 (`ConvTranspose2d`) 進行上採樣。\n",
    "  - 隱藏層使用 `ReLU` 激活函數，輸出層使用 `Tanh` 激活函數。\n",
    "  - 批標準化 (`BatchNorm2d`) 提供穩定的訓練。\n",
    "\n",
    "### Train the Generator\n",
    "- Generate Fake Images:\n",
    "  - Use the generator to create fake images: $x_{fake} = G(z)$.\n",
    "- Fool the Discriminator:\n",
    "  - Pass the fake images to the discriminator: $D(x_{fake})$.\n",
    "  - Compute the loss: $L_G = -\\log(D(x_{fake}))$.\n",
    "    - Note: Here, the generator wants $D(x_{fake})$ to be close to 1 (to fool the discriminator into thinking the images are real).\n",
    "- Update the Generator:\n",
    "  - Update the generator's weights to minimize $L_G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_bias = False\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is Z (latent vector)\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 3, bias=use_bias),  # Adjust padding to 3\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (ngf) x 28 x 28\n",
    "            nn.ConvTranspose2d(ngf, 1, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
    "            nn.Tanh()\n",
    "            # Output size: 1 x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 判別器 (Discriminator)\n",
    "- 功能：將輸入圖像（真實或生成）分類為真實 ( 1 ) 或假 ( 0 )。\n",
    "- 架構：\n",
    "  - 使用卷積層 (`Conv2d`) 進行下採樣。\n",
    "  - 批標準化 (`BatchNorm2d`) 與 LeakyReLU 激活函數穩定訓練並避免梯度稀疏。\n",
    "  - 最後一層使用 `Sigmoid` 激活函數進行`二分類`。\n",
    "\n",
    "#### Train the Discriminator\n",
    "- Input Real Images:\n",
    "  - Feed a batch of real images ( $X_{real}$) from the dataset to the discriminator.\n",
    "  - Compute the discriminator's output: $D(x_{ real })$, where $D$ is the discriminator.\n",
    "  - Calculate the loss: $L_{real} = - \\log (D(X_{real}))$.\n",
    "- Input Fake Images:\n",
    "  - Generate a batch of fake images ($x_{fake} = G(z)$) using the generator.\n",
    "  - Feed $x_{fake}$ to the discriminator.\n",
    "  - Compute the discriminator's output: $D(x_{fake})$.\n",
    "  - Calculate the loss: $L_{fake} = -\\log ( 1- D(x_{fake}))$. \n",
    "- Update the Discriminator:\n",
    "  - Combine the losses: $L_D = L_{real} + L_{fake}$.\n",
    "  - Update the discriminator's weights to minimize $L_D$​."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using LeakyReLU activation function to avoid sparse gradients\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 2, 1, bias=use_bias),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 損失函數與優化器 (Define Lossy function (BinaryCrossentropy))\n",
    "- 損失函數：使用二元交叉熵損失 (`BCELoss`)。\n",
    "- 優化器：Adam，學習率 0.0002，$\\beta=(0.5,0.999)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "optimizer_D = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_G = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model ( Epoch: 25 )\n",
    "- The GAN training alternates between updating the discriminator and the generator.\n",
    "- Alternate between training the discriminator and generator for many iterations.\n",
    "- Over time, the generator improves, creating images that become increasingly realistic, while the discriminator becomes better at distinguishing real from fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/938] Loss_D: 1.5097945928573608 Loss_G: 1.1543402671813965 D(x): 0.4472287893295288 D(G(z)): 0.45976877212524414/0.3312985301017761\n",
      "[0/25][50/938] Loss_D: 0.34454309940338135 Loss_G: 5.165733337402344 D(x): 0.9224661588668823 D(G(z)): 0.2222352921962738/0.00794902816414833\n",
      "[0/25][100/938] Loss_D: 0.6081126928329468 Loss_G: 3.144291400909424 D(x): 0.8790104389190674 D(G(z)): 0.35810741782188416/0.051959265023469925\n",
      "[0/25][150/938] Loss_D: 0.5404613018035889 Loss_G: 2.705753803253174 D(x): 0.7721208333969116 D(G(z)): 0.20771917700767517/0.09180039167404175\n",
      "[0/25][200/938] Loss_D: 0.4969502091407776 Loss_G: 2.1167471408843994 D(x): 0.737260103225708 D(G(z)): 0.15248596668243408/0.13881339132785797\n",
      "[0/25][250/938] Loss_D: 0.43428170680999756 Loss_G: 3.2315146923065186 D(x): 0.9031277894973755 D(G(z)): 0.26793742179870605/0.04626741260290146\n",
      "[0/25][300/938] Loss_D: 0.4345180094242096 Loss_G: 2.32780122756958 D(x): 0.7674660682678223 D(G(z)): 0.1359778642654419/0.13087432086467743\n",
      "[0/25][350/938] Loss_D: 0.6325154304504395 Loss_G: 1.8973100185394287 D(x): 0.7288739085197449 D(G(z)): 0.22399044036865234/0.17641206085681915\n",
      "[0/25][400/938] Loss_D: 0.4162819981575012 Loss_G: 2.840437889099121 D(x): 0.9090592861175537 D(G(z)): 0.26254788041114807/0.06650842726230621\n",
      "[0/25][450/938] Loss_D: 0.37257733941078186 Loss_G: 2.731576919555664 D(x): 0.902729332447052 D(G(z)): 0.2192506194114685/0.08412924408912659\n",
      "[0/25][500/938] Loss_D: 0.6697335243225098 Loss_G: 1.0459425449371338 D(x): 0.5889202356338501 D(G(z)): 0.07156798988580704/0.3839395046234131\n",
      "[0/25][550/938] Loss_D: 0.6476950645446777 Loss_G: 1.0298041105270386 D(x): 0.6543583273887634 D(G(z)): 0.14796607196331024/0.39007890224456787\n",
      "[0/25][600/938] Loss_D: 0.5889017581939697 Loss_G: 2.0286824703216553 D(x): 0.7605106234550476 D(G(z)): 0.2436819225549698/0.1563904881477356\n",
      "[0/25][650/938] Loss_D: 0.6805709600448608 Loss_G: 0.9478087425231934 D(x): 0.6128568649291992 D(G(z)): 0.1275625228881836/0.4154978096485138\n",
      "[0/25][700/938] Loss_D: 0.9679911136627197 Loss_G: 1.8299922943115234 D(x): 0.7358675003051758 D(G(z)): 0.45252689719200134/0.1846294105052948\n",
      "[0/25][750/938] Loss_D: 0.8165501356124878 Loss_G: 2.3692259788513184 D(x): 0.8487156629562378 D(G(z)): 0.4507216215133667/0.10648743808269501\n",
      "[0/25][800/938] Loss_D: 0.7593804597854614 Loss_G: 1.4122658967971802 D(x): 0.6490563750267029 D(G(z)): 0.2507133185863495/0.2634047269821167\n",
      "[0/25][850/938] Loss_D: 0.8154697418212891 Loss_G: 1.409766674041748 D(x): 0.6307336091995239 D(G(z)): 0.25325220823287964/0.26895472407341003\n",
      "[0/25][900/938] Loss_D: 0.8946714401245117 Loss_G: 2.097297430038452 D(x): 0.8276019096374512 D(G(z)): 0.4780399203300476/0.13923200964927673\n",
      "[1/25][0/938] Loss_D: 0.7672609090805054 Loss_G: 1.3997468948364258 D(x): 0.6542176604270935 D(G(z)): 0.25142958760261536/0.273617684841156\n",
      "[1/25][50/938] Loss_D: 0.7841231226921082 Loss_G: 1.119627594947815 D(x): 0.7142643928527832 D(G(z)): 0.30995234847068787/0.3560680150985718\n",
      "[1/25][100/938] Loss_D: 0.7850787043571472 Loss_G: 2.042332172393799 D(x): 0.7829161882400513 D(G(z)): 0.3865481913089752/0.15192165970802307\n",
      "[1/25][150/938] Loss_D: 1.168640375137329 Loss_G: 0.6894015669822693 D(x): 0.4340958595275879 D(G(z)): 0.1750958263874054/0.5280762910842896\n",
      "[1/25][200/938] Loss_D: 0.9226539134979248 Loss_G: 1.3588125705718994 D(x): 0.6628244519233704 D(G(z)): 0.36333167552948/0.2870137691497803\n",
      "[1/25][250/938] Loss_D: 0.8857502937316895 Loss_G: 1.3744378089904785 D(x): 0.6015019416809082 D(G(z)): 0.2653213143348694/0.27241259813308716\n",
      "[1/25][300/938] Loss_D: 0.8998737335205078 Loss_G: 1.4030110836029053 D(x): 0.648360013961792 D(G(z)): 0.33836302161216736/0.2743580937385559\n",
      "[1/25][350/938] Loss_D: 0.8837313652038574 Loss_G: 1.9362530708312988 D(x): 0.6947726011276245 D(G(z)): 0.3603042960166931/0.17197605967521667\n",
      "[1/25][400/938] Loss_D: 0.8443536162376404 Loss_G: 1.3912296295166016 D(x): 0.7027819156646729 D(G(z)): 0.339205265045166/0.2835688591003418\n",
      "[1/25][450/938] Loss_D: 1.0225656032562256 Loss_G: 0.750097393989563 D(x): 0.49378007650375366 D(G(z)): 0.2148890644311905/0.50031578540802\n",
      "[1/25][500/938] Loss_D: 0.948317289352417 Loss_G: 1.3149347305297852 D(x): 0.5608804225921631 D(G(z)): 0.22887174785137177/0.3160311281681061\n",
      "[1/25][550/938] Loss_D: 0.874086856842041 Loss_G: 1.3562169075012207 D(x): 0.6580520272254944 D(G(z)): 0.3241027891635895/0.2881336808204651\n",
      "[1/25][600/938] Loss_D: 0.8910334706306458 Loss_G: 1.6028255224227905 D(x): 0.8094807267189026 D(G(z)): 0.4642278552055359/0.22211632132530212\n",
      "[1/25][650/938] Loss_D: 0.9005012512207031 Loss_G: 2.896066427230835 D(x): 0.7875327467918396 D(G(z)): 0.4480794668197632/0.06590230762958527\n",
      "[1/25][700/938] Loss_D: 0.9781531095504761 Loss_G: 1.4724059104919434 D(x): 0.72493577003479 D(G(z)): 0.4324343800544739/0.2560819387435913\n",
      "[1/25][750/938] Loss_D: 0.9700640439987183 Loss_G: 0.9058894515037537 D(x): 0.5213286876678467 D(G(z)): 0.19978563487529755/0.44377681612968445\n",
      "[1/25][800/938] Loss_D: 0.9134981036186218 Loss_G: 1.9384980201721191 D(x): 0.7668181657791138 D(G(z)): 0.43858402967453003/0.1679500788450241\n",
      "[1/25][850/938] Loss_D: 0.8645499348640442 Loss_G: 1.4176777601242065 D(x): 0.5717326402664185 D(G(z)): 0.20599187910556793/0.2798253297805786\n",
      "[1/25][900/938] Loss_D: 1.012685775756836 Loss_G: 0.933496356010437 D(x): 0.5120013356208801 D(G(z)): 0.22796432673931122/0.41654646396636963\n",
      "[2/25][0/938] Loss_D: 0.9743354916572571 Loss_G: 1.1372320652008057 D(x): 0.5349871516227722 D(G(z)): 0.22670608758926392/0.3451993763446808\n",
      "[2/25][50/938] Loss_D: 0.8902599811553955 Loss_G: 1.5443427562713623 D(x): 0.6782145500183105 D(G(z)): 0.3481639623641968/0.25108247995376587\n",
      "[2/25][100/938] Loss_D: 1.1644916534423828 Loss_G: 0.8486140966415405 D(x): 0.44378501176834106 D(G(z)): 0.23675395548343658/0.4555363655090332\n",
      "[2/25][150/938] Loss_D: 0.9232427477836609 Loss_G: 1.576852798461914 D(x): 0.7076605558395386 D(G(z)): 0.3949352502822876/0.2353099286556244\n",
      "[2/25][200/938] Loss_D: 1.1627774238586426 Loss_G: 2.572455883026123 D(x): 0.7956429719924927 D(G(z)): 0.5350642204284668/0.09459729492664337\n",
      "[2/25][250/938] Loss_D: 1.2734999656677246 Loss_G: 0.7577238082885742 D(x): 0.41277945041656494 D(G(z)): 0.2368505299091339/0.49702948331832886\n",
      "[2/25][300/938] Loss_D: 0.9956828951835632 Loss_G: 2.071502685546875 D(x): 0.6646157503128052 D(G(z)): 0.39767080545425415/0.14792004227638245\n",
      "[2/25][350/938] Loss_D: 0.9166868925094604 Loss_G: 1.5235753059387207 D(x): 0.6396058797836304 D(G(z)): 0.3362087905406952/0.24083442986011505\n",
      "[2/25][400/938] Loss_D: 0.9615706205368042 Loss_G: 1.1105071306228638 D(x): 0.5605595707893372 D(G(z)): 0.2619035243988037/0.36365094780921936\n",
      "[2/25][450/938] Loss_D: 1.0742117166519165 Loss_G: 1.0171160697937012 D(x): 0.44528716802597046 D(G(z)): 0.15520599484443665/0.3933766484260559\n",
      "[2/25][500/938] Loss_D: 0.9619427919387817 Loss_G: 1.216138482093811 D(x): 0.4874984920024872 D(G(z)): 0.158279687166214/0.33139947056770325\n",
      "[2/25][550/938] Loss_D: 0.9552104473114014 Loss_G: 1.1219708919525146 D(x): 0.4925253391265869 D(G(z)): 0.14970844984054565/0.3746955394744873\n",
      "[2/25][600/938] Loss_D: 1.1304705142974854 Loss_G: 1.9412262439727783 D(x): 0.707366406917572 D(G(z)): 0.5002647638320923/0.16925111413002014\n",
      "[2/25][650/938] Loss_D: 0.9883707165718079 Loss_G: 1.8196566104888916 D(x): 0.7947209477424622 D(G(z)): 0.4860856533050537/0.18677470088005066\n",
      "[2/25][700/938] Loss_D: 1.1480807065963745 Loss_G: 1.0864670276641846 D(x): 0.5657486915588379 D(G(z)): 0.39556562900543213/0.36935940384864807\n",
      "[2/25][750/938] Loss_D: 0.838203489780426 Loss_G: 1.4961789846420288 D(x): 0.6153686046600342 D(G(z)): 0.2523998022079468/0.2533036768436432\n",
      "[2/25][800/938] Loss_D: 1.2025337219238281 Loss_G: 1.2674989700317383 D(x): 0.546441912651062 D(G(z)): 0.39257460832595825/0.3208792805671692\n",
      "[2/25][850/938] Loss_D: 0.9801055788993835 Loss_G: 1.4832794666290283 D(x): 0.7387357354164124 D(G(z)): 0.46137431263923645/0.254896342754364\n",
      "[2/25][900/938] Loss_D: 1.1938376426696777 Loss_G: 1.085963249206543 D(x): 0.5621402263641357 D(G(z)): 0.4070119261741638/0.3639611005783081\n",
      "[3/25][0/938] Loss_D: 1.2229572534561157 Loss_G: 0.9504047632217407 D(x): 0.4633084535598755 D(G(z)): 0.28476929664611816/0.4189072847366333\n",
      "[3/25][50/938] Loss_D: 0.8611871004104614 Loss_G: 1.387568473815918 D(x): 0.6294025182723999 D(G(z)): 0.2871541976928711/0.272758424282074\n",
      "[3/25][100/938] Loss_D: 0.9607467651367188 Loss_G: 1.7705209255218506 D(x): 0.7443264722824097 D(G(z)): 0.44317102432250977/0.19508428871631622\n",
      "[3/25][150/938] Loss_D: 1.1558079719543457 Loss_G: 0.989580512046814 D(x): 0.43791112303733826 D(G(z)): 0.17383205890655518/0.40583494305610657\n",
      "[3/25][200/938] Loss_D: 0.7939475774765015 Loss_G: 1.4550920724868774 D(x): 0.6758877038955688 D(G(z)): 0.2943626046180725/0.2676382064819336\n",
      "[3/25][250/938] Loss_D: 1.227976679801941 Loss_G: 1.2434180974960327 D(x): 0.612810492515564 D(G(z)): 0.4655473828315735/0.3136540651321411\n",
      "[3/25][300/938] Loss_D: 1.0671278238296509 Loss_G: 1.6061803102493286 D(x): 0.6890743374824524 D(G(z)): 0.45411768555641174/0.2322653830051422\n",
      "[3/25][350/938] Loss_D: 1.0885624885559082 Loss_G: 1.228158712387085 D(x): 0.5231310725212097 D(G(z)): 0.2975597381591797/0.3202102482318878\n",
      "[3/25][400/938] Loss_D: 1.0271714925765991 Loss_G: 1.6941092014312744 D(x): 0.7362725734710693 D(G(z)): 0.47146856784820557/0.2139381617307663\n",
      "[3/25][450/938] Loss_D: 0.9954472184181213 Loss_G: 1.2596203088760376 D(x): 0.5790861248970032 D(G(z)): 0.30889075994491577/0.31507694721221924\n",
      "[3/25][500/938] Loss_D: 0.9750208258628845 Loss_G: 1.47263503074646 D(x): 0.6580839157104492 D(G(z)): 0.39415639638900757/0.25160491466522217\n",
      "[3/25][550/938] Loss_D: 1.0142207145690918 Loss_G: 1.5095770359039307 D(x): 0.7633148431777954 D(G(z)): 0.49372398853302/0.2435685396194458\n",
      "[3/25][600/938] Loss_D: 1.134355068206787 Loss_G: 1.5122621059417725 D(x): 0.658996045589447 D(G(z)): 0.45973122119903564/0.24727430939674377\n",
      "[3/25][650/938] Loss_D: 0.9441838264465332 Loss_G: 1.5255320072174072 D(x): 0.6768363118171692 D(G(z)): 0.38081586360931396/0.24084092676639557\n",
      "[3/25][700/938] Loss_D: 1.254509687423706 Loss_G: 1.6757086515426636 D(x): 0.805880606174469 D(G(z)): 0.5959721803665161/0.22107362747192383\n",
      "[3/25][750/938] Loss_D: 1.125585675239563 Loss_G: 1.7271873950958252 D(x): 0.6921762228012085 D(G(z)): 0.4842410981655121/0.1985139548778534\n",
      "[3/25][800/938] Loss_D: 1.071735143661499 Loss_G: 1.128718614578247 D(x): 0.5289306640625 D(G(z)): 0.3020363450050354/0.35401245951652527\n",
      "[3/25][850/938] Loss_D: 1.0739622116088867 Loss_G: 1.0735045671463013 D(x): 0.5261942148208618 D(G(z)): 0.2843973934650421/0.3666034936904907\n",
      "[3/25][900/938] Loss_D: 0.7337943315505981 Loss_G: 2.3512401580810547 D(x): 0.7532694339752197 D(G(z)): 0.3345385789871216/0.11108123511075974\n",
      "[4/25][0/938] Loss_D: 0.9566062688827515 Loss_G: 1.4320659637451172 D(x): 0.6388120651245117 D(G(z)): 0.3593820333480835/0.272657573223114\n",
      "[4/25][50/938] Loss_D: 1.043651819229126 Loss_G: 1.070244550704956 D(x): 0.5224127769470215 D(G(z)): 0.27888259291648865/0.36930784583091736\n",
      "[4/25][100/938] Loss_D: 0.9298363924026489 Loss_G: 1.1267257928848267 D(x): 0.5608945488929749 D(G(z)): 0.24269893765449524/0.35355517268180847\n",
      "[4/25][150/938] Loss_D: 0.9155122637748718 Loss_G: 1.947205662727356 D(x): 0.7161833643913269 D(G(z)): 0.4051857590675354/0.1646432727575302\n",
      "[4/25][200/938] Loss_D: 1.011962652206421 Loss_G: 1.562747597694397 D(x): 0.6970269680023193 D(G(z)): 0.4443287253379822/0.23161913454532623\n",
      "[4/25][250/938] Loss_D: 0.9690864086151123 Loss_G: 1.4202638864517212 D(x): 0.7716923356056213 D(G(z)): 0.46130746603012085/0.27532535791397095\n",
      "[4/25][300/938] Loss_D: 0.8536996245384216 Loss_G: 1.3663721084594727 D(x): 0.6219773292541504 D(G(z)): 0.2659129500389099/0.2846377491950989\n",
      "[4/25][350/938] Loss_D: 1.081249713897705 Loss_G: 1.342806100845337 D(x): 0.6397420167922974 D(G(z)): 0.42497485876083374/0.28320014476776123\n",
      "[4/25][400/938] Loss_D: 0.9792792201042175 Loss_G: 1.7826074361801147 D(x): 0.6214884519577026 D(G(z)): 0.36041194200515747/0.18984292447566986\n",
      "[4/25][450/938] Loss_D: 1.1263487339019775 Loss_G: 1.1807258129119873 D(x): 0.59272700548172 D(G(z)): 0.40908101201057434/0.3290468454360962\n",
      "[4/25][500/938] Loss_D: 0.9543411731719971 Loss_G: 1.0828379392623901 D(x): 0.6141475439071655 D(G(z)): 0.32773396372795105/0.3678329288959503\n",
      "[4/25][550/938] Loss_D: 1.074353575706482 Loss_G: 1.1208839416503906 D(x): 0.5271288156509399 D(G(z)): 0.2967056930065155/0.36315426230430603\n",
      "[4/25][600/938] Loss_D: 1.0112605094909668 Loss_G: 1.1979477405548096 D(x): 0.5609897971153259 D(G(z)): 0.3098043203353882/0.3326892554759979\n",
      "[4/25][650/938] Loss_D: 1.1232599020004272 Loss_G: 1.8630038499832153 D(x): 0.763144850730896 D(G(z)): 0.5393717288970947/0.17531904578208923\n",
      "[4/25][700/938] Loss_D: 1.2887229919433594 Loss_G: 0.7481552362442017 D(x): 0.4752068519592285 D(G(z)): 0.36258620023727417/0.5029473900794983\n",
      "[4/25][750/938] Loss_D: 1.215110421180725 Loss_G: 1.2204073667526245 D(x): 0.47893577814102173 D(G(z)): 0.3093867003917694/0.32396751642227173\n",
      "[4/25][800/938] Loss_D: 1.0750575065612793 Loss_G: 1.3450889587402344 D(x): 0.550487756729126 D(G(z)): 0.32312026619911194/0.2862653434276581\n",
      "[4/25][850/938] Loss_D: 1.3398547172546387 Loss_G: 1.1322212219238281 D(x): 0.6011035442352295 D(G(z)): 0.501285970211029/0.3428027927875519\n",
      "[4/25][900/938] Loss_D: 1.2593638896942139 Loss_G: 1.1938557624816895 D(x): 0.5019705295562744 D(G(z)): 0.38199931383132935/0.3297666609287262\n",
      "[5/25][0/938] Loss_D: 0.954146146774292 Loss_G: 1.398467779159546 D(x): 0.626366376876831 D(G(z)): 0.34233948588371277/0.27500343322753906\n",
      "[5/25][50/938] Loss_D: 1.0802489519119263 Loss_G: 1.4543125629425049 D(x): 0.5559565424919128 D(G(z)): 0.32970690727233887/0.26621565222740173\n",
      "[5/25][100/938] Loss_D: 1.0107594728469849 Loss_G: 1.261749267578125 D(x): 0.6132768392562866 D(G(z)): 0.36414989829063416/0.314453661441803\n",
      "[5/25][150/938] Loss_D: 0.9866205453872681 Loss_G: 1.694397211074829 D(x): 0.6999738812446594 D(G(z)): 0.4201418459415436/0.21458423137664795\n",
      "[5/25][200/938] Loss_D: 1.0296285152435303 Loss_G: 1.1168078184127808 D(x): 0.49519771337509155 D(G(z)): 0.23420122265815735/0.35134685039520264\n",
      "[5/25][250/938] Loss_D: 1.2486839294433594 Loss_G: 1.0634677410125732 D(x): 0.5344059467315674 D(G(z)): 0.4147041440010071/0.37188732624053955\n",
      "[5/25][300/938] Loss_D: 1.102473497390747 Loss_G: 1.5128173828125 D(x): 0.6488232612609863 D(G(z)): 0.45599889755249023/0.24367240071296692\n",
      "[5/25][350/938] Loss_D: 1.1281942129135132 Loss_G: 1.9544165134429932 D(x): 0.6602679491043091 D(G(z)): 0.46653369069099426/0.16090849041938782\n",
      "[5/25][400/938] Loss_D: 1.2841105461120605 Loss_G: 1.3915557861328125 D(x): 0.6054835319519043 D(G(z)): 0.4945472180843353/0.2804948389530182\n",
      "[5/25][450/938] Loss_D: 1.031799554824829 Loss_G: 1.0044246912002563 D(x): 0.5825518369674683 D(G(z)): 0.3453177213668823/0.400479257106781\n",
      "[5/25][500/938] Loss_D: 0.8941325545310974 Loss_G: 1.1574631929397583 D(x): 0.6163931488990784 D(G(z)): 0.3043038547039032/0.3363129496574402\n",
      "[5/25][550/938] Loss_D: 0.9929215312004089 Loss_G: 1.3788182735443115 D(x): 0.5342974662780762 D(G(z)): 0.2633337378501892/0.2845417857170105\n",
      "[5/25][600/938] Loss_D: 0.963614821434021 Loss_G: 1.647223711013794 D(x): 0.6472800970077515 D(G(z)): 0.3584473133087158/0.23065230250358582\n",
      "[5/25][650/938] Loss_D: 1.2727595567703247 Loss_G: 1.3899563550949097 D(x): 0.6517825126647949 D(G(z)): 0.5180529356002808/0.2836647033691406\n",
      "[5/25][700/938] Loss_D: 1.14963960647583 Loss_G: 1.162217378616333 D(x): 0.5559942126274109 D(G(z)): 0.37955284118652344/0.3395676612854004\n",
      "[5/25][750/938] Loss_D: 0.9303295016288757 Loss_G: 1.2399346828460693 D(x): 0.544222354888916 D(G(z)): 0.23639211058616638/0.31944113969802856\n",
      "[5/25][800/938] Loss_D: 1.040475606918335 Loss_G: 1.3709311485290527 D(x): 0.5869690179824829 D(G(z)): 0.35874781012535095/0.2786419987678528\n",
      "[5/25][850/938] Loss_D: 1.06355881690979 Loss_G: 1.592566967010498 D(x): 0.622719407081604 D(G(z)): 0.3939301073551178/0.24077463150024414\n",
      "[5/25][900/938] Loss_D: 1.3600833415985107 Loss_G: 1.070175051689148 D(x): 0.47518807649612427 D(G(z)): 0.3951305150985718/0.3778287470340729\n",
      "[6/25][0/938] Loss_D: 1.4144963026046753 Loss_G: 1.1486483812332153 D(x): 0.6281776428222656 D(G(z)): 0.5233556032180786/0.3514036536216736\n",
      "[6/25][50/938] Loss_D: 1.1756446361541748 Loss_G: 1.2507840394973755 D(x): 0.5602151155471802 D(G(z)): 0.4046746492385864/0.3135033845901489\n",
      "[6/25][100/938] Loss_D: 1.0461980104446411 Loss_G: 1.4923404455184937 D(x): 0.6560883522033691 D(G(z)): 0.4292649030685425/0.24473491311073303\n",
      "[6/25][150/938] Loss_D: 1.0395236015319824 Loss_G: 1.2354724407196045 D(x): 0.6030110120773315 D(G(z)): 0.3741849660873413/0.3152804374694824\n",
      "[6/25][200/938] Loss_D: 1.1664996147155762 Loss_G: 1.0769095420837402 D(x): 0.5572335720062256 D(G(z)): 0.3871617615222931/0.3730353116989136\n",
      "[6/25][250/938] Loss_D: 1.0397260189056396 Loss_G: 1.3460562229156494 D(x): 0.645902693271637 D(G(z)): 0.41607117652893066/0.2930030822753906\n",
      "[6/25][300/938] Loss_D: 1.1159379482269287 Loss_G: 1.2235734462738037 D(x): 0.48757490515708923 D(G(z)): 0.25307440757751465/0.330657958984375\n",
      "[6/25][350/938] Loss_D: 1.1983555555343628 Loss_G: 1.5008440017700195 D(x): 0.7193567752838135 D(G(z)): 0.5364803671836853/0.25846004486083984\n",
      "[6/25][400/938] Loss_D: 1.0336705446243286 Loss_G: 1.3776097297668457 D(x): 0.6340442895889282 D(G(z)): 0.3922099173069/0.28276896476745605\n",
      "[6/25][450/938] Loss_D: 1.0814111232757568 Loss_G: 1.5503116846084595 D(x): 0.706997275352478 D(G(z)): 0.4914798140525818/0.23263435065746307\n",
      "[6/25][500/938] Loss_D: 0.9427036046981812 Loss_G: 1.390566349029541 D(x): 0.5750925540924072 D(G(z)): 0.2782439589500427/0.2814334034919739\n",
      "[6/25][550/938] Loss_D: 1.2375019788742065 Loss_G: 0.9958200454711914 D(x): 0.4838830828666687 D(G(z)): 0.32557588815689087/0.39868685603141785\n",
      "[6/25][600/938] Loss_D: 0.9014484882354736 Loss_G: 1.2233171463012695 D(x): 0.5288828611373901 D(G(z)): 0.1546986848115921/0.32576611638069153\n",
      "[6/25][650/938] Loss_D: 1.2468732595443726 Loss_G: 1.8150280714035034 D(x): 0.790425181388855 D(G(z)): 0.5816636085510254/0.2088472843170166\n",
      "[6/25][700/938] Loss_D: 1.0703532695770264 Loss_G: 1.1154029369354248 D(x): 0.5463768839836121 D(G(z)): 0.3203120231628418/0.35703134536743164\n",
      "[6/25][750/938] Loss_D: 1.278367042541504 Loss_G: 0.9352654218673706 D(x): 0.4328383207321167 D(G(z)): 0.2793959379196167/0.41848301887512207\n",
      "[6/25][800/938] Loss_D: 0.9565370082855225 Loss_G: 1.6380690336227417 D(x): 0.6508657932281494 D(G(z)): 0.3696579933166504/0.2226252406835556\n",
      "[6/25][850/938] Loss_D: 1.366734266281128 Loss_G: 0.9630299210548401 D(x): 0.45391806960105896 D(G(z)): 0.36996710300445557/0.4116293787956238\n",
      "[6/25][900/938] Loss_D: 1.1624820232391357 Loss_G: 1.1431776285171509 D(x): 0.5771685838699341 D(G(z)): 0.403009295463562/0.34697073698043823\n",
      "[7/25][0/938] Loss_D: 0.9786778688430786 Loss_G: 1.4842215776443481 D(x): 0.685119092464447 D(G(z)): 0.40907543897628784/0.2545376718044281\n",
      "[7/25][50/938] Loss_D: 1.1697137355804443 Loss_G: 1.1555259227752686 D(x): 0.6824088096618652 D(G(z)): 0.5043911933898926/0.34975796937942505\n",
      "[7/25][100/938] Loss_D: 1.0347938537597656 Loss_G: 1.313462257385254 D(x): 0.6324717402458191 D(G(z)): 0.39111584424972534/0.29749178886413574\n",
      "[7/25][150/938] Loss_D: 1.1181087493896484 Loss_G: 1.6437385082244873 D(x): 0.7810677289962769 D(G(z)): 0.5384658575057983/0.22154709696769714\n",
      "[7/25][200/938] Loss_D: 1.1764953136444092 Loss_G: 0.874264121055603 D(x): 0.48333579301834106 D(G(z)): 0.31258493661880493/0.45134297013282776\n",
      "[7/25][250/938] Loss_D: 0.9695745706558228 Loss_G: 1.560645580291748 D(x): 0.7091559171676636 D(G(z)): 0.4283214211463928/0.2388465702533722\n",
      "[7/25][300/938] Loss_D: 1.1790645122528076 Loss_G: 1.1616826057434082 D(x): 0.5171048641204834 D(G(z)): 0.3589271903038025/0.3472343683242798\n",
      "[7/25][350/938] Loss_D: 1.136458396911621 Loss_G: 1.3115582466125488 D(x): 0.5343941450119019 D(G(z)): 0.3369247317314148/0.3013284206390381\n",
      "[7/25][400/938] Loss_D: 0.8871872425079346 Loss_G: 1.6423041820526123 D(x): 0.7002045512199402 D(G(z)): 0.3750368058681488/0.21839626133441925\n",
      "[7/25][450/938] Loss_D: 1.1081054210662842 Loss_G: 1.3595407009124756 D(x): 0.5664980411529541 D(G(z)): 0.34910231828689575/0.2875699996948242\n",
      "[7/25][500/938] Loss_D: 0.8950474858283997 Loss_G: 1.4471256732940674 D(x): 0.5752463340759277 D(G(z)): 0.234601229429245/0.25819167494773865\n",
      "[7/25][550/938] Loss_D: 1.0522944927215576 Loss_G: 1.6659092903137207 D(x): 0.6391225457191467 D(G(z)): 0.41323912143707275/0.20826469361782074\n",
      "[7/25][600/938] Loss_D: 0.9303848147392273 Loss_G: 1.6069250106811523 D(x): 0.6891802549362183 D(G(z)): 0.3953675627708435/0.2277609258890152\n",
      "[7/25][650/938] Loss_D: 0.8986862301826477 Loss_G: 1.6879081726074219 D(x): 0.7504284381866455 D(G(z)): 0.4304240345954895/0.21062013506889343\n",
      "[7/25][700/938] Loss_D: 1.2631632089614868 Loss_G: 0.611177921295166 D(x): 0.4094846546649933 D(G(z)): 0.23943428695201874/0.5699772834777832\n",
      "[7/25][750/938] Loss_D: 1.0285272598266602 Loss_G: 1.6031081676483154 D(x): 0.5943645238876343 D(G(z)): 0.36299049854278564/0.224148690700531\n",
      "[7/25][800/938] Loss_D: 1.0938611030578613 Loss_G: 1.0126495361328125 D(x): 0.5235280990600586 D(G(z)): 0.297798216342926/0.3973780870437622\n",
      "[7/25][850/938] Loss_D: 0.9946655035018921 Loss_G: 1.3096277713775635 D(x): 0.6137300729751587 D(G(z)): 0.3401217460632324/0.31064391136169434\n",
      "[7/25][900/938] Loss_D: 1.2953780889511108 Loss_G: 0.9689466953277588 D(x): 0.4481015205383301 D(G(z)): 0.3182980418205261/0.4031147360801697\n",
      "[8/25][0/938] Loss_D: 0.9988096356391907 Loss_G: 1.6986360549926758 D(x): 0.682442843914032 D(G(z)): 0.428992360830307/0.20632588863372803\n",
      "[8/25][50/938] Loss_D: 1.0713903903961182 Loss_G: 1.0769124031066895 D(x): 0.5697330236434937 D(G(z)): 0.342549592256546/0.370426744222641\n",
      "[8/25][100/938] Loss_D: 1.222353219985962 Loss_G: 1.232609748840332 D(x): 0.48856502771377563 D(G(z)): 0.33421590924263/0.3341844379901886\n",
      "[8/25][150/938] Loss_D: 0.8824396133422852 Loss_G: 1.2669509649276733 D(x): 0.5839439034461975 D(G(z)): 0.25007644295692444/0.3107527494430542\n",
      "[8/25][200/938] Loss_D: 1.002467155456543 Loss_G: 1.4647173881530762 D(x): 0.6586966514587402 D(G(z)): 0.40350571274757385/0.2523709535598755\n",
      "[8/25][250/938] Loss_D: 0.9336357116699219 Loss_G: 1.240879774093628 D(x): 0.6024680137634277 D(G(z)): 0.29144683480262756/0.31739234924316406\n",
      "[8/25][300/938] Loss_D: 1.0595136880874634 Loss_G: 2.683964252471924 D(x): 0.6899923086166382 D(G(z)): 0.4551953077316284/0.08573000878095627\n",
      "[8/25][350/938] Loss_D: 1.0046913623809814 Loss_G: 1.0466443300247192 D(x): 0.5666762590408325 D(G(z)): 0.315731406211853/0.38566166162490845\n",
      "[8/25][400/938] Loss_D: 1.1482161283493042 Loss_G: 0.988155722618103 D(x): 0.46697086095809937 D(G(z)): 0.2635640501976013/0.3949812054634094\n",
      "[8/25][450/938] Loss_D: 1.0323753356933594 Loss_G: 1.1052969694137573 D(x): 0.6056620478630066 D(G(z)): 0.3576676845550537/0.3589857220649719\n",
      "[8/25][500/938] Loss_D: 1.2082723379135132 Loss_G: 1.155065655708313 D(x): 0.45860880613327026 D(G(z)): 0.26690712571144104/0.3445934057235718\n",
      "[8/25][550/938] Loss_D: 1.1493780612945557 Loss_G: 1.3067386150360107 D(x): 0.5741904973983765 D(G(z)): 0.3820265531539917/0.31297600269317627\n",
      "[8/25][600/938] Loss_D: 0.9183662533760071 Loss_G: 1.9930764436721802 D(x): 0.7284997701644897 D(G(z)): 0.41816431283950806/0.15442818403244019\n",
      "[8/25][650/938] Loss_D: 1.0190317630767822 Loss_G: 2.042127847671509 D(x): 0.7496819496154785 D(G(z)): 0.47768092155456543/0.15202409029006958\n",
      "[8/25][700/938] Loss_D: 1.0946979522705078 Loss_G: 1.2335408926010132 D(x): 0.6098287105560303 D(G(z)): 0.40749669075012207/0.32572150230407715\n",
      "[8/25][750/938] Loss_D: 1.0908966064453125 Loss_G: 1.2947860956192017 D(x): 0.48260143399238586 D(G(z)): 0.225448876619339/0.3043920695781708\n",
      "[8/25][800/938] Loss_D: 1.0963094234466553 Loss_G: 1.1244187355041504 D(x): 0.495199978351593 D(G(z)): 0.2720625400543213/0.36466795206069946\n",
      "[8/25][850/938] Loss_D: 1.0452091693878174 Loss_G: 2.167219638824463 D(x): 0.7444379329681396 D(G(z)): 0.47961917519569397/0.14017699658870697\n",
      "[8/25][900/938] Loss_D: 1.048531413078308 Loss_G: 1.3893628120422363 D(x): 0.6346961259841919 D(G(z)): 0.4029462933540344/0.2740904986858368\n",
      "[9/25][0/938] Loss_D: 1.2566277980804443 Loss_G: 1.0305545330047607 D(x): 0.5069611668586731 D(G(z)): 0.39517849683761597/0.3844261169433594\n",
      "[9/25][50/938] Loss_D: 1.0434664487838745 Loss_G: 1.6853067874908447 D(x): 0.6653002500534058 D(G(z)): 0.4420356750488281/0.20122498273849487\n",
      "[9/25][100/938] Loss_D: 1.2303853034973145 Loss_G: 1.1091747283935547 D(x): 0.5632846355438232 D(G(z)): 0.4250475764274597/0.3634929060935974\n",
      "[9/25][150/938] Loss_D: 1.100254774093628 Loss_G: 1.0989749431610107 D(x): 0.490327388048172 D(G(z)): 0.2525620460510254/0.36717280745506287\n",
      "[9/25][200/938] Loss_D: 1.0906832218170166 Loss_G: 1.441141128540039 D(x): 0.5591349601745605 D(G(z)): 0.3396144509315491/0.2623239755630493\n",
      "[9/25][250/938] Loss_D: 1.0229170322418213 Loss_G: 1.2736097574234009 D(x): 0.5893815159797668 D(G(z)): 0.35241973400115967/0.3052022457122803\n",
      "[9/25][300/938] Loss_D: 0.9090656042098999 Loss_G: 1.7644075155258179 D(x): 0.6993494033813477 D(G(z)): 0.38062453269958496/0.19352677464485168\n",
      "[9/25][350/938] Loss_D: 1.1438453197479248 Loss_G: 1.1358247995376587 D(x): 0.5032688975334167 D(G(z)): 0.3066112697124481/0.34654700756073\n",
      "[9/25][400/938] Loss_D: 1.1172488927841187 Loss_G: 1.441679835319519 D(x): 0.5944178104400635 D(G(z)): 0.40267473459243774/0.26657527685165405\n",
      "[9/25][450/938] Loss_D: 1.0693496465682983 Loss_G: 1.398796796798706 D(x): 0.6379276514053345 D(G(z)): 0.41956639289855957/0.2723560333251953\n",
      "[9/25][500/938] Loss_D: 0.9496101140975952 Loss_G: 1.6212491989135742 D(x): 0.6424283385276794 D(G(z)): 0.36061111092567444/0.21935658156871796\n",
      "[9/25][550/938] Loss_D: 0.9937639236450195 Loss_G: 1.559802770614624 D(x): 0.7715592384338379 D(G(z)): 0.4783916473388672/0.2414925992488861\n",
      "[9/25][600/938] Loss_D: 1.0255789756774902 Loss_G: 1.1404085159301758 D(x): 0.5443987846374512 D(G(z)): 0.30291125178337097/0.3534163236618042\n",
      "[9/25][650/938] Loss_D: 0.926689624786377 Loss_G: 1.5361955165863037 D(x): 0.6425720453262329 D(G(z)): 0.341066837310791/0.24557189643383026\n",
      "[9/25][700/938] Loss_D: 0.8678951263427734 Loss_G: 1.314831256866455 D(x): 0.6123270392417908 D(G(z)): 0.2572556138038635/0.3150269091129303\n",
      "[9/25][750/938] Loss_D: 1.0355751514434814 Loss_G: 1.0883759260177612 D(x): 0.6055367588996887 D(G(z)): 0.34981483221054077/0.37162715196609497\n",
      "[9/25][800/938] Loss_D: 1.030294418334961 Loss_G: 1.2026724815368652 D(x): 0.6074897050857544 D(G(z)): 0.3548770546913147/0.3298463523387909\n",
      "[9/25][850/938] Loss_D: 1.061887264251709 Loss_G: 1.3851518630981445 D(x): 0.6672385334968567 D(G(z)): 0.44127142429351807/0.2733287513256073\n",
      "[9/25][900/938] Loss_D: 1.078568696975708 Loss_G: 1.497391700744629 D(x): 0.7479867935180664 D(G(z)): 0.5117430090904236/0.2437475621700287\n",
      "[10/25][0/938] Loss_D: 1.1724590063095093 Loss_G: 1.1621496677398682 D(x): 0.5513569116592407 D(G(z)): 0.3608299493789673/0.34710627794265747\n",
      "[10/25][50/938] Loss_D: 1.1411690711975098 Loss_G: 0.8856048583984375 D(x): 0.4801725149154663 D(G(z)): 0.2464907318353653/0.44440510869026184\n",
      "[10/25][100/938] Loss_D: 0.9842127561569214 Loss_G: 1.0970532894134521 D(x): 0.5339136719703674 D(G(z)): 0.25092625617980957/0.3684587776660919\n",
      "[10/25][150/938] Loss_D: 1.1198480129241943 Loss_G: 1.537414789199829 D(x): 0.6361764669418335 D(G(z)): 0.43883195519447327/0.24713806807994843\n",
      "[10/25][200/938] Loss_D: 1.0916203260421753 Loss_G: 0.945341944694519 D(x): 0.4332035779953003 D(G(z)): 0.14773276448249817/0.4277350902557373\n",
      "[10/25][250/938] Loss_D: 1.096735954284668 Loss_G: 1.473041296005249 D(x): 0.682705283164978 D(G(z)): 0.45709097385406494/0.2606361508369446\n",
      "[10/25][300/938] Loss_D: 1.2756414413452148 Loss_G: 0.7873362302780151 D(x): 0.4364283084869385 D(G(z)): 0.26519525051116943/0.48535779118537903\n",
      "[10/25][350/938] Loss_D: 0.9962774515151978 Loss_G: 1.7758973836898804 D(x): 0.598558783531189 D(G(z)): 0.3379107713699341/0.19139422476291656\n",
      "[10/25][400/938] Loss_D: 1.0161412954330444 Loss_G: 1.5016846656799316 D(x): 0.6382238268852234 D(G(z)): 0.38286933302879333/0.2532753348350525\n",
      "[10/25][450/938] Loss_D: 0.9239581823348999 Loss_G: 1.5845001935958862 D(x): 0.714767575263977 D(G(z)): 0.3999808430671692/0.23858299851417542\n",
      "[10/25][500/938] Loss_D: 0.8699908256530762 Loss_G: 1.388471245765686 D(x): 0.6065326929092407 D(G(z)): 0.26430365443229675/0.2886807918548584\n",
      "[10/25][550/938] Loss_D: 0.9792592525482178 Loss_G: 1.7645070552825928 D(x): 0.7820277810096741 D(G(z)): 0.48142722249031067/0.19775335490703583\n",
      "[10/25][600/938] Loss_D: 0.8538315296173096 Loss_G: 1.527629017829895 D(x): 0.6345809698104858 D(G(z)): 0.28639543056488037/0.2563197612762451\n",
      "[10/25][650/938] Loss_D: 1.2305959463119507 Loss_G: 1.1603484153747559 D(x): 0.5585751533508301 D(G(z)): 0.40416499972343445/0.3438345193862915\n",
      "[10/25][700/938] Loss_D: 1.031996488571167 Loss_G: 1.0168946981430054 D(x): 0.5562866926193237 D(G(z)): 0.30255958437919617/0.40076613426208496\n",
      "[10/25][750/938] Loss_D: 0.944348931312561 Loss_G: 1.2195689678192139 D(x): 0.6265163421630859 D(G(z)): 0.3401358425617218/0.3217376172542572\n",
      "[10/25][800/938] Loss_D: 0.9161579608917236 Loss_G: 1.9492548704147339 D(x): 0.7566893100738525 D(G(z)): 0.4303738474845886/0.1697632074356079\n",
      "[10/25][850/938] Loss_D: 1.0409904718399048 Loss_G: 1.279724359512329 D(x): 0.5970126390457153 D(G(z)): 0.357393354177475/0.3052767813205719\n",
      "[10/25][900/938] Loss_D: 0.982966423034668 Loss_G: 1.3066825866699219 D(x): 0.6227841973304749 D(G(z)): 0.355729877948761/0.2985675632953644\n",
      "[11/25][0/938] Loss_D: 1.114822506904602 Loss_G: 1.079305648803711 D(x): 0.48864275217056274 D(G(z)): 0.26256808638572693/0.3735935389995575\n",
      "[11/25][50/938] Loss_D: 1.3022326231002808 Loss_G: 1.5814237594604492 D(x): 0.8274583220481873 D(G(z)): 0.63786780834198/0.23464655876159668\n",
      "[11/25][100/938] Loss_D: 1.0303599834442139 Loss_G: 1.0899708271026611 D(x): 0.5581659078598022 D(G(z)): 0.32317638397216797/0.3631119430065155\n",
      "[11/25][150/938] Loss_D: 1.1660065650939941 Loss_G: 1.0297459363937378 D(x): 0.4772786498069763 D(G(z)): 0.2989962100982666/0.38472044467926025\n",
      "[11/25][200/938] Loss_D: 0.9625742435455322 Loss_G: 1.679917812347412 D(x): 0.7617116570472717 D(G(z)): 0.4555882215499878/0.21219006180763245\n",
      "[11/25][250/938] Loss_D: 1.1951744556427002 Loss_G: 1.6823792457580566 D(x): 0.7063978314399719 D(G(z)): 0.5377838611602783/0.20938251912593842\n",
      "[11/25][300/938] Loss_D: 0.8649587035179138 Loss_G: 1.2875759601593018 D(x): 0.6084422469139099 D(G(z)): 0.2728237509727478/0.3039087951183319\n",
      "[11/25][350/938] Loss_D: 0.9032934308052063 Loss_G: 1.4825520515441895 D(x): 0.6362810134887695 D(G(z)): 0.3238893747329712/0.26319026947021484\n",
      "[11/25][400/938] Loss_D: 0.8917647004127502 Loss_G: 1.591378927230835 D(x): 0.7030240893363953 D(G(z)): 0.3790055513381958/0.22977948188781738\n",
      "[11/25][450/938] Loss_D: 0.8033254146575928 Loss_G: 1.855025291442871 D(x): 0.7696801424026489 D(G(z)): 0.3824552595615387/0.17858421802520752\n",
      "[11/25][500/938] Loss_D: 1.4307061433792114 Loss_G: 0.9590534567832947 D(x): 0.463151752948761 D(G(z)): 0.4070563018321991/0.4106045961380005\n",
      "[11/25][550/938] Loss_D: 0.8425400257110596 Loss_G: 1.6904879808425903 D(x): 0.63694167137146 D(G(z)): 0.2902945280075073/0.20184458792209625\n",
      "[11/25][600/938] Loss_D: 0.8125127553939819 Loss_G: 1.7325016260147095 D(x): 0.6561817526817322 D(G(z)): 0.26903295516967773/0.20012030005455017\n",
      "[11/25][650/938] Loss_D: 0.9731588363647461 Loss_G: 1.5310728549957275 D(x): 0.6684587001800537 D(G(z)): 0.39725542068481445/0.24393057823181152\n",
      "[11/25][700/938] Loss_D: 1.1097118854522705 Loss_G: 1.4057376384735107 D(x): 0.7284595966339111 D(G(z)): 0.49796146154403687/0.28071892261505127\n",
      "[11/25][750/938] Loss_D: 0.9752339124679565 Loss_G: 1.4479458332061768 D(x): 0.6918740272521973 D(G(z)): 0.40973320603370667/0.2672401964664459\n",
      "[11/25][800/938] Loss_D: 1.0474700927734375 Loss_G: 1.512786626815796 D(x): 0.6347750425338745 D(G(z)): 0.39304739236831665/0.2660273015499115\n",
      "[11/25][850/938] Loss_D: 1.1768795251846313 Loss_G: 1.0655815601348877 D(x): 0.4977681636810303 D(G(z)): 0.3135496973991394/0.3806034326553345\n",
      "[11/25][900/938] Loss_D: 0.9526286125183105 Loss_G: 2.1330161094665527 D(x): 0.7209291458129883 D(G(z)): 0.42833226919174194/0.14317527413368225\n",
      "[12/25][0/938] Loss_D: 1.1234047412872314 Loss_G: 1.538034200668335 D(x): 0.5440866947174072 D(G(z)): 0.3331552743911743/0.25295358896255493\n",
      "[12/25][50/938] Loss_D: 0.9768989682197571 Loss_G: 1.8387243747711182 D(x): 0.6781725883483887 D(G(z)): 0.4017820954322815/0.1914910078048706\n",
      "[12/25][100/938] Loss_D: 1.1948227882385254 Loss_G: 1.7528934478759766 D(x): 0.8206405639648438 D(G(z)): 0.593350350856781/0.20029842853546143\n",
      "[12/25][150/938] Loss_D: 0.8369532227516174 Loss_G: 1.8506790399551392 D(x): 0.746113657951355 D(G(z)): 0.3810509443283081/0.17933139204978943\n",
      "[12/25][200/938] Loss_D: 1.0469117164611816 Loss_G: 1.1276087760925293 D(x): 0.4505643844604492 D(G(z)): 0.15665462613105774/0.35895413160324097\n",
      "[12/25][250/938] Loss_D: 1.2578556537628174 Loss_G: 2.0688412189483643 D(x): 0.6943638324737549 D(G(z)): 0.5450408458709717/0.1525954306125641\n",
      "[12/25][300/938] Loss_D: 1.1138430833816528 Loss_G: 1.616705298423767 D(x): 0.6699846386909485 D(G(z)): 0.47793132066726685/0.2226276695728302\n",
      "[12/25][350/938] Loss_D: 0.8944299817085266 Loss_G: 1.2469935417175293 D(x): 0.5947742462158203 D(G(z)): 0.26471656560897827/0.3159899413585663\n",
      "[12/25][400/938] Loss_D: 1.1299595832824707 Loss_G: 1.6609704494476318 D(x): 0.6934807896614075 D(G(z)): 0.48057639598846436/0.22731448709964752\n",
      "[12/25][450/938] Loss_D: 1.0343763828277588 Loss_G: 1.6283365488052368 D(x): 0.6675670742988586 D(G(z)): 0.42510557174682617/0.22450752556324005\n",
      "[12/25][500/938] Loss_D: 1.0123918056488037 Loss_G: 1.4359019994735718 D(x): 0.6403945684432983 D(G(z)): 0.3902122974395752/0.26291513442993164\n",
      "[12/25][550/938] Loss_D: 1.2716262340545654 Loss_G: 1.719178557395935 D(x): 0.7931481599807739 D(G(z)): 0.6005060076713562/0.21111580729484558\n",
      "[12/25][600/938] Loss_D: 1.0988068580627441 Loss_G: 1.6329143047332764 D(x): 0.6259889602661133 D(G(z)): 0.4221067428588867/0.21658173203468323\n",
      "[12/25][650/938] Loss_D: 0.9943333864212036 Loss_G: 1.5664172172546387 D(x): 0.639889121055603 D(G(z)): 0.3854846656322479/0.22890999913215637\n",
      "[12/25][700/938] Loss_D: 1.078246831893921 Loss_G: 1.346076488494873 D(x): 0.7307521104812622 D(G(z)): 0.47504082322120667/0.2995232045650482\n",
      "[12/25][750/938] Loss_D: 0.9776206016540527 Loss_G: 2.151540756225586 D(x): 0.7646505236625671 D(G(z)): 0.4724607467651367/0.13615646958351135\n",
      "[12/25][800/938] Loss_D: 1.1651930809020996 Loss_G: 1.2436425685882568 D(x): 0.549727737903595 D(G(z)): 0.37455177307128906/0.3248695731163025\n",
      "[12/25][850/938] Loss_D: 0.9868682622909546 Loss_G: 1.4940683841705322 D(x): 0.6770657300949097 D(G(z)): 0.4067589044570923/0.25929751992225647\n",
      "[12/25][900/938] Loss_D: 1.0313737392425537 Loss_G: 1.4551020860671997 D(x): 0.7338145971298218 D(G(z)): 0.4743536710739136/0.25885164737701416\n",
      "[13/25][0/938] Loss_D: 0.9094911813735962 Loss_G: 1.1476969718933105 D(x): 0.5794107913970947 D(G(z)): 0.2349492460489273/0.3497356176376343\n",
      "[13/25][50/938] Loss_D: 0.8679959177970886 Loss_G: 1.9865208864212036 D(x): 0.704770028591156 D(G(z)): 0.3624829947948456/0.1651139259338379\n",
      "[13/25][100/938] Loss_D: 0.9820311665534973 Loss_G: 1.071852684020996 D(x): 0.5954200625419617 D(G(z)): 0.32272887229919434/0.3731924593448639\n",
      "[13/25][150/938] Loss_D: 1.136457920074463 Loss_G: 1.1438567638397217 D(x): 0.5384582281112671 D(G(z)): 0.3328554034233093/0.35292530059814453\n",
      "[13/25][200/938] Loss_D: 0.9542195796966553 Loss_G: 1.8473451137542725 D(x): 0.6727774739265442 D(G(z)): 0.38535523414611816/0.18684935569763184\n",
      "[13/25][250/938] Loss_D: 1.3511382341384888 Loss_G: 1.307149887084961 D(x): 0.5564334392547607 D(G(z)): 0.4698581397533417/0.3003266453742981\n",
      "[13/25][300/938] Loss_D: 1.333290457725525 Loss_G: 1.4579896926879883 D(x): 0.690015435218811 D(G(z)): 0.5734623670578003/0.26036131381988525\n",
      "[13/25][350/938] Loss_D: 0.8514719009399414 Loss_G: 1.5329469442367554 D(x): 0.7541972398757935 D(G(z)): 0.4046310782432556/0.24196992814540863\n",
      "[13/25][400/938] Loss_D: 1.2672429084777832 Loss_G: 1.3036165237426758 D(x): 0.533356785774231 D(G(z)): 0.41262733936309814/0.29719579219818115\n",
      "[13/25][450/938] Loss_D: 0.9763842225074768 Loss_G: 1.4074411392211914 D(x): 0.618373453617096 D(G(z)): 0.342587411403656/0.26610270142555237\n",
      "[13/25][500/938] Loss_D: 0.9515790939331055 Loss_G: 1.6876033544540405 D(x): 0.6334351301193237 D(G(z)): 0.3244149088859558/0.2207697331905365\n",
      "[13/25][550/938] Loss_D: 0.9685356616973877 Loss_G: 1.236568570137024 D(x): 0.5586062073707581 D(G(z)): 0.2663916051387787/0.32757627964019775\n",
      "[13/25][600/938] Loss_D: 0.967149019241333 Loss_G: 1.317643642425537 D(x): 0.5216162800788879 D(G(z)): 0.19963234663009644/0.3201909065246582\n",
      "[13/25][650/938] Loss_D: 0.9977407455444336 Loss_G: 2.3089823722839355 D(x): 0.8251520395278931 D(G(z)): 0.5070028901100159/0.12162160873413086\n",
      "[13/25][700/938] Loss_D: 0.9324934482574463 Loss_G: 1.1024670600891113 D(x): 0.5596655011177063 D(G(z)): 0.23364120721817017/0.36702919006347656\n",
      "[13/25][750/938] Loss_D: 0.8414536714553833 Loss_G: 1.7473223209381104 D(x): 0.6937364339828491 D(G(z)): 0.3319529891014099/0.20863144099712372\n",
      "[13/25][800/938] Loss_D: 1.2147471904754639 Loss_G: 1.2650102376937866 D(x): 0.5040736198425293 D(G(z)): 0.33512577414512634/0.3219093084335327\n",
      "[13/25][850/938] Loss_D: 1.3242303133010864 Loss_G: 0.9724068641662598 D(x): 0.5159648060798645 D(G(z)): 0.427878201007843/0.41707003116607666\n",
      "[13/25][900/938] Loss_D: 0.8814718127250671 Loss_G: 1.4252727031707764 D(x): 0.6533533334732056 D(G(z)): 0.3288547694683075/0.2691698670387268\n",
      "[14/25][0/938] Loss_D: 1.0484846830368042 Loss_G: 1.7451602220535278 D(x): 0.7963427305221558 D(G(z)): 0.5065215826034546/0.21338176727294922\n",
      "[14/25][50/938] Loss_D: 0.9026724696159363 Loss_G: 1.376817226409912 D(x): 0.6725252866744995 D(G(z)): 0.3553670048713684/0.2738650441169739\n",
      "[14/25][100/938] Loss_D: 1.0185233354568481 Loss_G: 1.8741523027420044 D(x): 0.7037684917449951 D(G(z)): 0.43871885538101196/0.1754571497440338\n",
      "[14/25][150/938] Loss_D: 0.9036705493927002 Loss_G: 1.9146215915679932 D(x): 0.6060417890548706 D(G(z)): 0.2759891152381897/0.18136392533779144\n",
      "[14/25][200/938] Loss_D: 1.149010181427002 Loss_G: 1.1602413654327393 D(x): 0.5116028785705566 D(G(z)): 0.31025946140289307/0.3467554450035095\n",
      "[14/25][250/938] Loss_D: 1.0695247650146484 Loss_G: 1.4283891916275024 D(x): 0.561366081237793 D(G(z)): 0.31797489523887634/0.27105361223220825\n",
      "[14/25][300/938] Loss_D: 0.8968745470046997 Loss_G: 1.7759143114089966 D(x): 0.7740260362625122 D(G(z)): 0.4345276951789856/0.19594025611877441\n",
      "[14/25][350/938] Loss_D: 0.8743534088134766 Loss_G: 1.4598984718322754 D(x): 0.7089645266532898 D(G(z)): 0.36818650364875793/0.2562433183193207\n",
      "[14/25][400/938] Loss_D: 0.889414370059967 Loss_G: 1.8881738185882568 D(x): 0.6309536695480347 D(G(z)): 0.295369952917099/0.17671796679496765\n",
      "[14/25][450/938] Loss_D: 1.103217601776123 Loss_G: 0.9101594686508179 D(x): 0.4902256429195404 D(G(z)): 0.22897747159004211/0.447296679019928\n",
      "[14/25][500/938] Loss_D: 0.8145543336868286 Loss_G: 1.5322074890136719 D(x): 0.6449369192123413 D(G(z)): 0.2614257335662842/0.25084951519966125\n",
      "[14/25][550/938] Loss_D: 0.9054431319236755 Loss_G: 1.209010362625122 D(x): 0.586745023727417 D(G(z)): 0.25233179330825806/0.33706632256507874\n",
      "[14/25][600/938] Loss_D: 1.0344724655151367 Loss_G: 1.304051160812378 D(x): 0.561362624168396 D(G(z)): 0.3000817894935608/0.30242979526519775\n",
      "[14/25][650/938] Loss_D: 1.1233727931976318 Loss_G: 1.7500461339950562 D(x): 0.6081889867782593 D(G(z)): 0.4062890410423279/0.20054318010807037\n",
      "[14/25][700/938] Loss_D: 1.0087807178497314 Loss_G: 1.7819757461547852 D(x): 0.7013322710990906 D(G(z)): 0.4343736171722412/0.20348533987998962\n",
      "[14/25][750/938] Loss_D: 0.9188219904899597 Loss_G: 1.2728990316390991 D(x): 0.6458949446678162 D(G(z)): 0.3315039873123169/0.3116859495639801\n",
      "[14/25][800/938] Loss_D: 1.0433179140090942 Loss_G: 1.4885541200637817 D(x): 0.6846503615379333 D(G(z)): 0.43367600440979004/0.26137593388557434\n",
      "[14/25][850/938] Loss_D: 0.8091864585876465 Loss_G: 1.560831904411316 D(x): 0.7177317142486572 D(G(z)): 0.3365100622177124/0.23373180627822876\n",
      "[14/25][900/938] Loss_D: 1.0585739612579346 Loss_G: 0.9608926773071289 D(x): 0.4896673262119293 D(G(z)): 0.23153841495513916/0.40910303592681885\n",
      "[15/25][0/938] Loss_D: 1.3497438430786133 Loss_G: 2.4278430938720703 D(x): 0.8538661599159241 D(G(z)): 0.6242476105690002/0.12705999612808228\n",
      "[15/25][50/938] Loss_D: 1.1642069816589355 Loss_G: 2.173029899597168 D(x): 0.7945468425750732 D(G(z)): 0.5623253583908081/0.13616305589675903\n",
      "[15/25][100/938] Loss_D: 0.9756755828857422 Loss_G: 1.1587951183319092 D(x): 0.5381053686141968 D(G(z)): 0.2186826467514038/0.3489503264427185\n",
      "[15/25][150/938] Loss_D: 1.1137349605560303 Loss_G: 1.9358665943145752 D(x): 0.6927800178527832 D(G(z)): 0.4687235355377197/0.17220447957515717\n",
      "[15/25][200/938] Loss_D: 0.7352474927902222 Loss_G: 2.204343318939209 D(x): 0.7374176979064941 D(G(z)): 0.31039080023765564/0.13547120988368988\n",
      "[15/25][250/938] Loss_D: 0.9540209770202637 Loss_G: 1.132677674293518 D(x): 0.5565682053565979 D(G(z)): 0.2632400691509247/0.355551153421402\n",
      "[15/25][300/938] Loss_D: 1.3015947341918945 Loss_G: 1.1980531215667725 D(x): 0.5609066486358643 D(G(z)): 0.4589273929595947/0.3324713706970215\n",
      "[15/25][350/938] Loss_D: 0.9238927364349365 Loss_G: 1.1520259380340576 D(x): 0.5450700521469116 D(G(z)): 0.216172456741333/0.3493461012840271\n",
      "[15/25][400/938] Loss_D: 1.0987988710403442 Loss_G: 1.249112606048584 D(x): 0.5861431360244751 D(G(z)): 0.37274253368377686/0.3310401737689972\n",
      "[15/25][450/938] Loss_D: 0.7311347126960754 Loss_G: 1.8041877746582031 D(x): 0.7353358268737793 D(G(z)): 0.30631494522094727/0.19847869873046875\n",
      "[15/25][500/938] Loss_D: 1.0628472566604614 Loss_G: 1.9210329055786133 D(x): 0.7356144785881042 D(G(z)): 0.4723478853702545/0.17632970213890076\n",
      "[15/25][550/938] Loss_D: 1.0725854635238647 Loss_G: 1.4743120670318604 D(x): 0.5749063491821289 D(G(z)): 0.33624911308288574/0.27074313163757324\n",
      "[15/25][600/938] Loss_D: 0.9494105577468872 Loss_G: 1.3433635234832764 D(x): 0.5848277807235718 D(G(z)): 0.2740299701690674/0.29480212926864624\n",
      "[15/25][650/938] Loss_D: 0.9067454934120178 Loss_G: 1.6342735290527344 D(x): 0.7590568661689758 D(G(z)): 0.4295459985733032/0.22369486093521118\n",
      "[15/25][700/938] Loss_D: 0.740562915802002 Loss_G: 1.5234436988830566 D(x): 0.6618069410324097 D(G(z)): 0.23325055837631226/0.25216537714004517\n",
      "[15/25][750/938] Loss_D: 0.9341806769371033 Loss_G: 1.56809401512146 D(x): 0.6928906440734863 D(G(z)): 0.39430105686187744/0.23666226863861084\n",
      "[15/25][800/938] Loss_D: 1.154362440109253 Loss_G: 1.4680511951446533 D(x): 0.621697187423706 D(G(z)): 0.4293098747730255/0.26326674222946167\n",
      "[15/25][850/938] Loss_D: 1.1874189376831055 Loss_G: 2.0409164428710938 D(x): 0.6876833438873291 D(G(z)): 0.5056489706039429/0.15115460753440857\n",
      "[15/25][900/938] Loss_D: 1.2086397409439087 Loss_G: 2.0738813877105713 D(x): 0.8123061656951904 D(G(z)): 0.5971083641052246/0.1447526216506958\n",
      "[16/25][0/938] Loss_D: 0.9420913457870483 Loss_G: 1.4672131538391113 D(x): 0.6258928775787354 D(G(z)): 0.32512181997299194/0.25847750902175903\n",
      "[16/25][50/938] Loss_D: 1.0608206987380981 Loss_G: 1.9107003211975098 D(x): 0.8286139965057373 D(G(z)): 0.5335378646850586/0.17690694332122803\n",
      "[16/25][100/938] Loss_D: 0.7779732346534729 Loss_G: 1.3080217838287354 D(x): 0.620392382144928 D(G(z)): 0.21526005864143372/0.30315208435058594\n",
      "[16/25][150/938] Loss_D: 0.983642041683197 Loss_G: 1.4469883441925049 D(x): 0.6022435426712036 D(G(z)): 0.3238487243652344/0.2679404616355896\n",
      "[16/25][200/938] Loss_D: 0.8947958946228027 Loss_G: 1.7534513473510742 D(x): 0.7390900254249573 D(G(z)): 0.4060382843017578/0.19744375348091125\n",
      "[16/25][250/938] Loss_D: 0.9643360376358032 Loss_G: 1.413151741027832 D(x): 0.6279112100601196 D(G(z)): 0.31389832496643066/0.2740233242511749\n",
      "[16/25][300/938] Loss_D: 1.1299999952316284 Loss_G: 1.522637128829956 D(x): 0.6054431200027466 D(G(z)): 0.4072979688644409/0.2483484148979187\n",
      "[16/25][350/938] Loss_D: 1.001333236694336 Loss_G: 1.5613605976104736 D(x): 0.5869361162185669 D(G(z)): 0.31466788053512573/0.24357563257217407\n",
      "[16/25][400/938] Loss_D: 0.8933452367782593 Loss_G: 1.5279062986373901 D(x): 0.6233874559402466 D(G(z)): 0.29392728209495544/0.2525763511657715\n",
      "[16/25][450/938] Loss_D: 1.0880202054977417 Loss_G: 1.3196156024932861 D(x): 0.4309651851654053 D(G(z)): 0.13247306644916534/0.31049615144729614\n",
      "[16/25][500/938] Loss_D: 0.9328346252441406 Loss_G: 1.1688860654830933 D(x): 0.5453563332557678 D(G(z)): 0.2231336534023285/0.3370423913002014\n",
      "[16/25][550/938] Loss_D: 1.0287996530532837 Loss_G: 2.252201557159424 D(x): 0.7442692518234253 D(G(z)): 0.4570799469947815/0.13258376717567444\n",
      "[16/25][600/938] Loss_D: 0.8159803748130798 Loss_G: 1.6077604293823242 D(x): 0.5364534854888916 D(G(z)): 0.10123980790376663/0.24576768279075623\n",
      "[16/25][650/938] Loss_D: 0.9976348876953125 Loss_G: 1.5946121215820312 D(x): 0.5922911167144775 D(G(z)): 0.3111964464187622/0.22904151678085327\n",
      "[16/25][700/938] Loss_D: 1.089870572090149 Loss_G: 2.225891590118408 D(x): 0.7662892937660217 D(G(z)): 0.5078049898147583/0.13439594209194183\n",
      "[16/25][750/938] Loss_D: 0.9577697515487671 Loss_G: 1.3853284120559692 D(x): 0.6708433032035828 D(G(z)): 0.3893161416053772/0.2788519263267517\n",
      "[16/25][800/938] Loss_D: 0.8843623399734497 Loss_G: 1.7691742181777954 D(x): 0.6265408396720886 D(G(z)): 0.2833070158958435/0.20333969593048096\n",
      "[16/25][850/938] Loss_D: 0.7824194431304932 Loss_G: 2.017662286758423 D(x): 0.6808788776397705 D(G(z)): 0.28674638271331787/0.17445683479309082\n",
      "[16/25][900/938] Loss_D: 0.9435521364212036 Loss_G: 1.7239993810653687 D(x): 0.6470922231674194 D(G(z)): 0.321891725063324/0.22961340844631195\n",
      "[17/25][0/938] Loss_D: 0.7984552383422852 Loss_G: 1.768049716949463 D(x): 0.7092463970184326 D(G(z)): 0.32209545373916626/0.21850380301475525\n",
      "[17/25][50/938] Loss_D: 0.8285822868347168 Loss_G: 1.729576587677002 D(x): 0.6565687656402588 D(G(z)): 0.27238279581069946/0.2122134268283844\n",
      "[17/25][100/938] Loss_D: 0.7692689895629883 Loss_G: 1.9504504203796387 D(x): 0.7220773696899414 D(G(z)): 0.3194873332977295/0.16320857405662537\n",
      "[17/25][150/938] Loss_D: 1.054218053817749 Loss_G: 1.6157894134521484 D(x): 0.5619536638259888 D(G(z)): 0.3272801637649536/0.23468557000160217\n",
      "[17/25][200/938] Loss_D: 1.19950270652771 Loss_G: 1.1514184474945068 D(x): 0.42961835861206055 D(G(z)): 0.18847230076789856/0.3612193465232849\n",
      "[17/25][250/938] Loss_D: 1.005340337753296 Loss_G: 1.3579938411712646 D(x): 0.5766074657440186 D(G(z)): 0.3101608157157898/0.29077595472335815\n",
      "[17/25][300/938] Loss_D: 0.926786482334137 Loss_G: 1.5063087940216064 D(x): 0.723634660243988 D(G(z)): 0.4039098024368286/0.2583177387714386\n",
      "[17/25][350/938] Loss_D: 0.8820716738700867 Loss_G: 1.5563089847564697 D(x): 0.6571717858314514 D(G(z)): 0.33313995599746704/0.2404053956270218\n",
      "[17/25][400/938] Loss_D: 0.9443700909614563 Loss_G: 1.8587486743927002 D(x): 0.8177664279937744 D(G(z)): 0.48874711990356445/0.17712020874023438\n",
      "[17/25][450/938] Loss_D: 1.050520896911621 Loss_G: 1.0578575134277344 D(x): 0.49517107009887695 D(G(z)): 0.2271585613489151/0.3879461884498596\n",
      "[17/25][500/938] Loss_D: 0.8723801374435425 Loss_G: 1.5522558689117432 D(x): 0.6091418266296387 D(G(z)): 0.2499362826347351/0.2539059817790985\n",
      "[17/25][550/938] Loss_D: 1.2949514389038086 Loss_G: 1.0726559162139893 D(x): 0.4462841749191284 D(G(z)): 0.2877722382545471/0.4038199186325073\n",
      "[17/25][600/938] Loss_D: 1.022965908050537 Loss_G: 1.2762527465820312 D(x): 0.537473201751709 D(G(z)): 0.25842100381851196/0.32592374086380005\n",
      "[17/25][650/938] Loss_D: 0.8828006386756897 Loss_G: 1.4528287649154663 D(x): 0.6818311214447021 D(G(z)): 0.353189080953598/0.2619263529777527\n",
      "[17/25][700/938] Loss_D: 0.8575031757354736 Loss_G: 1.6360859870910645 D(x): 0.7446098327636719 D(G(z)): 0.38509485125541687/0.22748889029026031\n",
      "[17/25][750/938] Loss_D: 1.1328456401824951 Loss_G: 1.4759536981582642 D(x): 0.5953474640846252 D(G(z)): 0.40431609749794006/0.26454460620880127\n",
      "[17/25][800/938] Loss_D: 1.428242802619934 Loss_G: 0.8801504373550415 D(x): 0.35299667716026306 D(G(z)): 0.19515320658683777/0.4629612863063812\n",
      "[17/25][850/938] Loss_D: 0.8091623783111572 Loss_G: 1.6791703701019287 D(x): 0.7123429775238037 D(G(z)): 0.3409164547920227/0.20920735597610474\n",
      "[17/25][900/938] Loss_D: 1.190645694732666 Loss_G: 2.0251431465148926 D(x): 0.7307603359222412 D(G(z)): 0.5310181379318237/0.16702529788017273\n",
      "[18/25][0/938] Loss_D: 0.8634339570999146 Loss_G: 1.425485372543335 D(x): 0.6051821708679199 D(G(z)): 0.22270748019218445/0.2965713143348694\n",
      "[18/25][50/938] Loss_D: 0.7549009323120117 Loss_G: 1.2518291473388672 D(x): 0.7419670820236206 D(G(z)): 0.3136495351791382/0.3325560390949249\n",
      "[18/25][100/938] Loss_D: 0.9879037141799927 Loss_G: 1.7141709327697754 D(x): 0.6180518269538879 D(G(z)): 0.33917051553726196/0.21871156990528107\n",
      "[18/25][150/938] Loss_D: 0.8192355632781982 Loss_G: 1.6207849979400635 D(x): 0.7294889688491821 D(G(z)): 0.3525257706642151/0.22344571352005005\n",
      "[18/25][200/938] Loss_D: 0.7837342619895935 Loss_G: 2.299133777618408 D(x): 0.7727862596511841 D(G(z)): 0.3791554570198059/0.12151124328374863\n",
      "[18/25][250/938] Loss_D: 1.043499231338501 Loss_G: 1.6211154460906982 D(x): 0.6897473335266113 D(G(z)): 0.4191129803657532/0.24465878307819366\n",
      "[18/25][300/938] Loss_D: 1.219065546989441 Loss_G: 0.9685664176940918 D(x): 0.45284783840179443 D(G(z)): 0.2638435363769531/0.40365374088287354\n",
      "[18/25][350/938] Loss_D: 1.0245850086212158 Loss_G: 1.7021889686584473 D(x): 0.6224753260612488 D(G(z)): 0.3650937080383301/0.22241230309009552\n",
      "[18/25][400/938] Loss_D: 1.1324666738510132 Loss_G: 1.4843502044677734 D(x): 0.5581901669502258 D(G(z)): 0.35070309042930603/0.26945486664772034\n",
      "[18/25][450/938] Loss_D: 0.8506889939308167 Loss_G: 2.599928617477417 D(x): 0.8036600351333618 D(G(z)): 0.42096954584121704/0.09822002798318863\n",
      "[18/25][500/938] Loss_D: 0.7326680421829224 Loss_G: 2.015246629714966 D(x): 0.7029656171798706 D(G(z)): 0.2680701017379761/0.16535578668117523\n",
      "[18/25][550/938] Loss_D: 0.8867435455322266 Loss_G: 1.5636141300201416 D(x): 0.6601129770278931 D(G(z)): 0.32705244421958923/0.2361912876367569\n",
      "[18/25][600/938] Loss_D: 1.2374978065490723 Loss_G: 1.1097898483276367 D(x): 0.4849335551261902 D(G(z)): 0.3185471296310425/0.37465783953666687\n",
      "[18/25][650/938] Loss_D: 1.0462599992752075 Loss_G: 1.7960364818572998 D(x): 0.723976194858551 D(G(z)): 0.47774359583854675/0.1871817409992218\n",
      "[18/25][700/938] Loss_D: 0.9943374395370483 Loss_G: 1.69546377658844 D(x): 0.6540236473083496 D(G(z)): 0.3771286606788635/0.21099430322647095\n",
      "[18/25][750/938] Loss_D: 0.8570444583892822 Loss_G: 1.3194776773452759 D(x): 0.6357004046440125 D(G(z)): 0.2885492444038391/0.3066406846046448\n",
      "[18/25][800/938] Loss_D: 0.992155909538269 Loss_G: 1.5413817167282104 D(x): 0.5988794565200806 D(G(z)): 0.313859224319458/0.24399062991142273\n",
      "[18/25][850/938] Loss_D: 1.1046230792999268 Loss_G: 1.005577564239502 D(x): 0.5298315286636353 D(G(z)): 0.2992122173309326/0.404455304145813\n",
      "[18/25][900/938] Loss_D: 0.7630254030227661 Loss_G: 2.272326946258545 D(x): 0.6804512739181519 D(G(z)): 0.26229679584503174/0.1281377375125885\n",
      "[19/25][0/938] Loss_D: 0.9060578346252441 Loss_G: 2.000615119934082 D(x): 0.7725731134414673 D(G(z)): 0.4261420965194702/0.16230548918247223\n",
      "[19/25][50/938] Loss_D: 0.8745852708816528 Loss_G: 1.9035574197769165 D(x): 0.6981865167617798 D(G(z)): 0.3533670902252197/0.18848711252212524\n",
      "[19/25][100/938] Loss_D: 0.9807865619659424 Loss_G: 1.5689713954925537 D(x): 0.6815133094787598 D(G(z)): 0.3886605501174927/0.24534833431243896\n",
      "[19/25][150/938] Loss_D: 0.9287155866622925 Loss_G: 1.4267117977142334 D(x): 0.6542632579803467 D(G(z)): 0.339057981967926/0.27966082096099854\n",
      "[19/25][200/938] Loss_D: 0.8058503866195679 Loss_G: 1.7019734382629395 D(x): 0.7025113701820374 D(G(z)): 0.312867671251297/0.2184184491634369\n",
      "[19/25][250/938] Loss_D: 1.2840608358383179 Loss_G: 1.9062981605529785 D(x): 0.8167996406555176 D(G(z)): 0.5913405418395996/0.1914810687303543\n",
      "[19/25][300/938] Loss_D: 0.7420706152915955 Loss_G: 1.5756456851959229 D(x): 0.6561461687088013 D(G(z)): 0.22995154559612274/0.23776575922966003\n",
      "[19/25][350/938] Loss_D: 0.8935554027557373 Loss_G: 1.6387465000152588 D(x): 0.6271926164627075 D(G(z)): 0.2978084981441498/0.22870782017707825\n",
      "[19/25][400/938] Loss_D: 0.9815657734870911 Loss_G: 1.5563421249389648 D(x): 0.6022576093673706 D(G(z)): 0.2998197674751282/0.24548369646072388\n",
      "[19/25][450/938] Loss_D: 0.8674553632736206 Loss_G: 1.5672156810760498 D(x): 0.6113974452018738 D(G(z)): 0.260186105966568/0.23889422416687012\n",
      "[19/25][500/938] Loss_D: 0.8324387073516846 Loss_G: 1.7128124237060547 D(x): 0.6377438306808472 D(G(z)): 0.27129626274108887/0.20446345210075378\n",
      "[19/25][550/938] Loss_D: 0.7621252536773682 Loss_G: 1.467794418334961 D(x): 0.6270918846130371 D(G(z)): 0.21170349419116974/0.2636975646018982\n",
      "[19/25][600/938] Loss_D: 0.8110570311546326 Loss_G: 2.091169834136963 D(x): 0.7589331865310669 D(G(z)): 0.36925774812698364/0.14409586787223816\n",
      "[19/25][650/938] Loss_D: 1.1384484767913818 Loss_G: 1.094355583190918 D(x): 0.4627978801727295 D(G(z)): 0.2194942831993103/0.387442946434021\n",
      "[19/25][700/938] Loss_D: 0.679938554763794 Loss_G: 1.5061118602752686 D(x): 0.6811612248420715 D(G(z)): 0.21854597330093384/0.25562989711761475\n",
      "[19/25][750/938] Loss_D: 0.7600893378257751 Loss_G: 2.015423536300659 D(x): 0.7142642736434937 D(G(z)): 0.2969967722892761/0.15823021531105042\n",
      "[19/25][800/938] Loss_D: 0.9068813323974609 Loss_G: 2.163130283355713 D(x): 0.8077743053436279 D(G(z)): 0.45540374517440796/0.13776007294654846\n",
      "[19/25][850/938] Loss_D: 0.9388343095779419 Loss_G: 1.543921709060669 D(x): 0.6203718185424805 D(G(z)): 0.2993531823158264/0.2555203139781952\n",
      "[19/25][900/938] Loss_D: 1.2122910022735596 Loss_G: 1.8672503232955933 D(x): 0.7548110485076904 D(G(z)): 0.5493625998497009/0.18794573843479156\n",
      "[20/25][0/938] Loss_D: 1.3772013187408447 Loss_G: 2.93448543548584 D(x): 0.8854831457138062 D(G(z)): 0.6727427244186401/0.07099278271198273\n",
      "[20/25][50/938] Loss_D: 1.2221496105194092 Loss_G: 1.4224662780761719 D(x): 0.5908013582229614 D(G(z)): 0.42718642950057983/0.2759600877761841\n",
      "[20/25][100/938] Loss_D: 0.7633188962936401 Loss_G: 1.426163911819458 D(x): 0.6435670852661133 D(G(z)): 0.23380222916603088/0.2761068344116211\n",
      "[20/25][150/938] Loss_D: 0.8651561737060547 Loss_G: 1.3105494976043701 D(x): 0.5565197467803955 D(G(z)): 0.16651993989944458/0.31452542543411255\n",
      "[20/25][200/938] Loss_D: 0.902084231376648 Loss_G: 1.844684362411499 D(x): 0.6830992698669434 D(G(z)): 0.355659544467926/0.1938038319349289\n",
      "[20/25][250/938] Loss_D: 1.0604021549224854 Loss_G: 1.742513656616211 D(x): 0.7975388765335083 D(G(z)): 0.5151006579399109/0.20590345561504364\n",
      "[20/25][300/938] Loss_D: 0.8017463684082031 Loss_G: 1.607067584991455 D(x): 0.6620550155639648 D(G(z)): 0.27500319480895996/0.2391381710767746\n",
      "[20/25][350/938] Loss_D: 0.9393259286880493 Loss_G: 1.3555339574813843 D(x): 0.6520164608955383 D(G(z)): 0.3519667983055115/0.29496562480926514\n",
      "[20/25][400/938] Loss_D: 0.7137081623077393 Loss_G: 1.9848263263702393 D(x): 0.7283899188041687 D(G(z)): 0.28881561756134033/0.17397166788578033\n",
      "[20/25][450/938] Loss_D: 1.0503909587860107 Loss_G: 3.239382028579712 D(x): 0.8684660196304321 D(G(z)): 0.5476884841918945/0.05060559883713722\n",
      "[20/25][500/938] Loss_D: 1.0429890155792236 Loss_G: 1.516672968864441 D(x): 0.6141135692596436 D(G(z)): 0.3523467481136322/0.24690648913383484\n",
      "[20/25][550/938] Loss_D: 1.3163281679153442 Loss_G: 1.1063292026519775 D(x): 0.5014296174049377 D(G(z)): 0.39237233996391296/0.373744934797287\n",
      "[20/25][600/938] Loss_D: 0.8848657608032227 Loss_G: 1.6306248903274536 D(x): 0.6516879200935364 D(G(z)): 0.31918710470199585/0.23685920238494873\n",
      "[20/25][650/938] Loss_D: 0.8930345773696899 Loss_G: 1.3851187229156494 D(x): 0.5647343397140503 D(G(z)): 0.20552483201026917/0.2860715687274933\n",
      "[20/25][700/938] Loss_D: 0.9123882055282593 Loss_G: 1.3598659038543701 D(x): 0.6703091263771057 D(G(z)): 0.350628137588501/0.2937690317630768\n",
      "[20/25][750/938] Loss_D: 1.0887824296951294 Loss_G: 1.1707946062088013 D(x): 0.43597614765167236 D(G(z)): 0.12972261011600494/0.35598379373550415\n",
      "[20/25][800/938] Loss_D: 0.9932233691215515 Loss_G: 1.959856390953064 D(x): 0.8115662336349487 D(G(z)): 0.4912444055080414/0.18200531601905823\n",
      "[20/25][850/938] Loss_D: 1.041137933731079 Loss_G: 1.5421940088272095 D(x): 0.6503167152404785 D(G(z)): 0.39238274097442627/0.25233280658721924\n",
      "[20/25][900/938] Loss_D: 0.9545959234237671 Loss_G: 1.2041434049606323 D(x): 0.574211835861206 D(G(z)): 0.2492925375699997/0.3376685380935669\n",
      "[21/25][0/938] Loss_D: 1.1973929405212402 Loss_G: 1.3388557434082031 D(x): 0.3783867359161377 D(G(z)): 0.06375660002231598/0.3243789076805115\n",
      "[21/25][50/938] Loss_D: 1.020597219467163 Loss_G: 1.0618385076522827 D(x): 0.4855770766735077 D(G(z)): 0.15847447514533997/0.3976471424102783\n",
      "[21/25][100/938] Loss_D: 0.9508794546127319 Loss_G: 2.020214557647705 D(x): 0.7628891468048096 D(G(z)): 0.4331822395324707/0.16529634594917297\n",
      "[21/25][150/938] Loss_D: 0.8293941020965576 Loss_G: 2.0888383388519287 D(x): 0.7998831272125244 D(G(z)): 0.401434063911438/0.14952650666236877\n",
      "[21/25][200/938] Loss_D: 1.0934512615203857 Loss_G: 1.8864754438400269 D(x): 0.6486797332763672 D(G(z)): 0.40361690521240234/0.1954299509525299\n",
      "[21/25][250/938] Loss_D: 0.806581437587738 Loss_G: 1.5752508640289307 D(x): 0.6806132793426514 D(G(z)): 0.2993457019329071/0.25765907764434814\n",
      "[21/25][300/938] Loss_D: 0.8109617829322815 Loss_G: 2.293867588043213 D(x): 0.7268680930137634 D(G(z)): 0.3393796682357788/0.1379508376121521\n",
      "[21/25][350/938] Loss_D: 0.9521454572677612 Loss_G: 1.2857354879379272 D(x): 0.6324570178985596 D(G(z)): 0.31631988286972046/0.3267357051372528\n",
      "[21/25][400/938] Loss_D: 0.8839454650878906 Loss_G: 1.886916160583496 D(x): 0.7451164126396179 D(G(z)): 0.4006665349006653/0.17995649576187134\n",
      "[21/25][450/938] Loss_D: 0.7519291043281555 Loss_G: 1.334403395652771 D(x): 0.6278287172317505 D(G(z)): 0.20457594096660614/0.2978881597518921\n",
      "[21/25][500/938] Loss_D: 1.1520354747772217 Loss_G: 0.9795215129852295 D(x): 0.40801119804382324 D(G(z)): 0.11886341869831085/0.4466651380062103\n",
      "[21/25][550/938] Loss_D: 0.9736099243164062 Loss_G: 1.4484654664993286 D(x): 0.6146166920661926 D(G(z)): 0.29335522651672363/0.2905856668949127\n",
      "[21/25][600/938] Loss_D: 0.8491748571395874 Loss_G: 1.5859839916229248 D(x): 0.6978002190589905 D(G(z)): 0.33078745007514954/0.24300219118595123\n",
      "[21/25][650/938] Loss_D: 0.9128644466400146 Loss_G: 1.7614657878875732 D(x): 0.6344296336174011 D(G(z)): 0.3054923415184021/0.20534765720367432\n",
      "[21/25][700/938] Loss_D: 0.7397788763046265 Loss_G: 1.7641925811767578 D(x): 0.6933366060256958 D(G(z)): 0.27103760838508606/0.20076045393943787\n",
      "[21/25][750/938] Loss_D: 0.9601238965988159 Loss_G: 1.6888885498046875 D(x): 0.7042928338050842 D(G(z)): 0.3916653096675873/0.22972699999809265\n",
      "[21/25][800/938] Loss_D: 0.9049845933914185 Loss_G: 1.4453099966049194 D(x): 0.6122749447822571 D(G(z)): 0.28733140230178833/0.28952038288116455\n",
      "[21/25][850/938] Loss_D: 0.7967888116836548 Loss_G: 2.1550846099853516 D(x): 0.7378445863723755 D(G(z)): 0.34005239605903625/0.14447468519210815\n",
      "[21/25][900/938] Loss_D: 1.0965231657028198 Loss_G: 1.4455697536468506 D(x): 0.5834429264068604 D(G(z)): 0.3702654540538788/0.27447453141212463\n",
      "[22/25][0/938] Loss_D: 1.0895562171936035 Loss_G: 1.3876926898956299 D(x): 0.5620959401130676 D(G(z)): 0.31960034370422363/0.2908947169780731\n",
      "[22/25][50/938] Loss_D: 0.9540805220603943 Loss_G: 1.0980861186981201 D(x): 0.5840815305709839 D(G(z)): 0.26417678594589233/0.36336129903793335\n",
      "[22/25][100/938] Loss_D: 0.8143202066421509 Loss_G: 1.314441204071045 D(x): 0.6043075323104858 D(G(z)): 0.2016475796699524/0.31583133339881897\n",
      "[22/25][150/938] Loss_D: 1.2762086391448975 Loss_G: 1.0016443729400635 D(x): 0.46220940351486206 D(G(z)): 0.2963096499443054/0.41031894087791443\n",
      "[22/25][200/938] Loss_D: 0.8491029739379883 Loss_G: 1.3344820737838745 D(x): 0.6281386613845825 D(G(z)): 0.2672129273414612/0.29461294412612915\n",
      "[22/25][250/938] Loss_D: 0.8029916286468506 Loss_G: 1.7537801265716553 D(x): 0.6741960048675537 D(G(z)): 0.2845776677131653/0.20848365128040314\n",
      "[22/25][300/938] Loss_D: 0.8225390911102295 Loss_G: 1.9718351364135742 D(x): 0.7837626338005066 D(G(z)): 0.3971291184425354/0.16315670311450958\n",
      "[22/25][350/938] Loss_D: 0.8889790773391724 Loss_G: 1.990078330039978 D(x): 0.6973938345909119 D(G(z)): 0.3564564287662506/0.16925472021102905\n",
      "[22/25][400/938] Loss_D: 1.1753215789794922 Loss_G: 2.5805516242980957 D(x): 0.7520360350608826 D(G(z)): 0.5139036774635315/0.10641977190971375\n",
      "[22/25][450/938] Loss_D: 0.797478973865509 Loss_G: 1.5438172817230225 D(x): 0.7318453788757324 D(G(z)): 0.34293481707572937/0.2484397143125534\n",
      "[22/25][500/938] Loss_D: 1.0115103721618652 Loss_G: 1.4052274227142334 D(x): 0.5041893124580383 D(G(z)): 0.19247391819953918/0.2854864299297333\n",
      "[22/25][550/938] Loss_D: 0.86882483959198 Loss_G: 1.4481027126312256 D(x): 0.6619282960891724 D(G(z)): 0.31634193658828735/0.261669397354126\n",
      "[22/25][600/938] Loss_D: 1.1683558225631714 Loss_G: 2.208728313446045 D(x): 0.8024176955223083 D(G(z)): 0.5481927394866943/0.1396452784538269\n",
      "[22/25][650/938] Loss_D: 0.9111378192901611 Loss_G: 1.7880247831344604 D(x): 0.7287787199020386 D(G(z)): 0.38670700788497925/0.20838484168052673\n",
      "[22/25][700/938] Loss_D: 0.8472118973731995 Loss_G: 1.7093045711517334 D(x): 0.7241397500038147 D(G(z)): 0.35491642355918884/0.22062116861343384\n",
      "[22/25][750/938] Loss_D: 0.8932068347930908 Loss_G: 1.728900671005249 D(x): 0.76186603307724 D(G(z)): 0.40267983078956604/0.21977220475673676\n",
      "[22/25][800/938] Loss_D: 1.1865944862365723 Loss_G: 1.3888952732086182 D(x): 0.48843687772750854 D(G(z)): 0.2809258997440338/0.2989916205406189\n",
      "[22/25][850/938] Loss_D: 0.9081599712371826 Loss_G: 1.3886373043060303 D(x): 0.6277088522911072 D(G(z)): 0.2882702946662903/0.2962096333503723\n",
      "[22/25][900/938] Loss_D: 0.8695257902145386 Loss_G: 1.7903428077697754 D(x): 0.7193799614906311 D(G(z)): 0.34839513897895813/0.20700640976428986\n",
      "[23/25][0/938] Loss_D: 0.7527037858963013 Loss_G: 2.025489568710327 D(x): 0.7778488397598267 D(G(z)): 0.34416547417640686/0.1632830798625946\n",
      "[23/25][50/938] Loss_D: 0.8478231430053711 Loss_G: 1.440517544746399 D(x): 0.5542181730270386 D(G(z)): 0.1563156694173813/0.27354779839515686\n",
      "[23/25][100/938] Loss_D: 1.0304908752441406 Loss_G: 1.164756178855896 D(x): 0.5141355395317078 D(G(z)): 0.20227164030075073/0.3532637059688568\n",
      "[23/25][150/938] Loss_D: 0.7052925229072571 Loss_G: 2.0411691665649414 D(x): 0.7309365272521973 D(G(z)): 0.2803596258163452/0.16788163781166077\n",
      "[23/25][200/938] Loss_D: 0.7433543801307678 Loss_G: 1.6099331378936768 D(x): 0.7161798477172852 D(G(z)): 0.29472675919532776/0.2339942455291748\n",
      "[23/25][250/938] Loss_D: 0.9650044441223145 Loss_G: 2.194983959197998 D(x): 0.7578107118606567 D(G(z)): 0.45151299238204956/0.13844329118728638\n",
      "[23/25][300/938] Loss_D: 1.0787286758422852 Loss_G: 2.2773079872131348 D(x): 0.77506422996521 D(G(z)): 0.49524641036987305/0.12826916575431824\n",
      "[23/25][350/938] Loss_D: 0.9288851022720337 Loss_G: 1.7016677856445312 D(x): 0.7107745409011841 D(G(z)): 0.3745047450065613/0.22837106883525848\n",
      "[23/25][400/938] Loss_D: 0.9253543615341187 Loss_G: 1.4525665044784546 D(x): 0.618793785572052 D(G(z)): 0.3075233995914459/0.2739662528038025\n",
      "[23/25][450/938] Loss_D: 0.6084962487220764 Loss_G: 1.6412084102630615 D(x): 0.7234524488449097 D(G(z)): 0.21061070263385773/0.22341632843017578\n",
      "[23/25][500/938] Loss_D: 0.724942684173584 Loss_G: 1.8535032272338867 D(x): 0.7301041483879089 D(G(z)): 0.29460179805755615/0.20033134520053864\n",
      "[23/25][550/938] Loss_D: 0.9380881786346436 Loss_G: 1.4065731763839722 D(x): 0.5795019865036011 D(G(z)): 0.25961482524871826/0.29680556058883667\n",
      "[23/25][600/938] Loss_D: 0.9231532216072083 Loss_G: 1.426807165145874 D(x): 0.630268394947052 D(G(z)): 0.3075491786003113/0.28086182475090027\n",
      "[23/25][650/938] Loss_D: 0.6521167755126953 Loss_G: 2.148160219192505 D(x): 0.7806887626647949 D(G(z)): 0.2845996618270874/0.14905712008476257\n",
      "[23/25][700/938] Loss_D: 1.1025002002716064 Loss_G: 1.1889523267745972 D(x): 0.496016263961792 D(G(z)): 0.22881841659545898/0.36934417486190796\n",
      "[23/25][750/938] Loss_D: 0.8669158220291138 Loss_G: 1.5904115438461304 D(x): 0.6207830905914307 D(G(z)): 0.25057291984558105/0.25296419858932495\n",
      "[23/25][800/938] Loss_D: 1.1303701400756836 Loss_G: 1.2768629789352417 D(x): 0.5989220142364502 D(G(z)): 0.3920598030090332/0.32286548614501953\n",
      "[23/25][850/938] Loss_D: 0.902977705001831 Loss_G: 2.010493755340576 D(x): 0.7351890206336975 D(G(z)): 0.3937194347381592/0.1653016060590744\n",
      "[23/25][900/938] Loss_D: 1.1784923076629639 Loss_G: 0.9316670894622803 D(x): 0.5659616589546204 D(G(z)): 0.3676820397377014/0.44363683462142944\n",
      "[24/25][0/938] Loss_D: 0.6693134307861328 Loss_G: 1.670427918434143 D(x): 0.6810508966445923 D(G(z)): 0.19110602140426636/0.2317812293767929\n",
      "[24/25][50/938] Loss_D: 0.8399326205253601 Loss_G: 1.678208351135254 D(x): 0.6865869760513306 D(G(z)): 0.32373884320259094/0.23061048984527588\n",
      "[24/25][100/938] Loss_D: 1.2070527076721191 Loss_G: 0.9275532960891724 D(x): 0.42549705505371094 D(G(z)): 0.19461975991725922/0.4414357841014862\n",
      "[24/25][150/938] Loss_D: 0.8517875671386719 Loss_G: 1.5146605968475342 D(x): 0.6024507284164429 D(G(z)): 0.2104209065437317/0.26769348978996277\n",
      "[24/25][200/938] Loss_D: 0.9772294163703918 Loss_G: 1.2251029014587402 D(x): 0.48236462473869324 D(G(z)): 0.1385338008403778/0.3539241552352905\n",
      "[24/25][250/938] Loss_D: 0.7945166826248169 Loss_G: 1.7311367988586426 D(x): 0.7450677156448364 D(G(z)): 0.34397393465042114/0.21537667512893677\n",
      "[24/25][300/938] Loss_D: 0.8793134093284607 Loss_G: 1.7707879543304443 D(x): 0.5926798582077026 D(G(z)): 0.22898158431053162/0.2202446162700653\n",
      "[24/25][350/938] Loss_D: 1.1349711418151855 Loss_G: 0.9432671666145325 D(x): 0.4806724190711975 D(G(z)): 0.23055051267147064/0.42526084184646606\n",
      "[24/25][400/938] Loss_D: 1.066328525543213 Loss_G: 1.9753937721252441 D(x): 0.6322980523109436 D(G(z)): 0.39115092158317566/0.16500705480575562\n",
      "[24/25][450/938] Loss_D: 0.794044554233551 Loss_G: 1.8244268894195557 D(x): 0.6230772733688354 D(G(z)): 0.22998324036598206/0.18049077689647675\n",
      "[24/25][500/938] Loss_D: 0.9689406752586365 Loss_G: 2.437117099761963 D(x): 0.7897031307220459 D(G(z)): 0.4637430012226105/0.1062069684267044\n",
      "[24/25][550/938] Loss_D: 0.9839909076690674 Loss_G: 2.038309335708618 D(x): 0.7709369659423828 D(G(z)): 0.4559888243675232/0.1676017940044403\n",
      "[24/25][600/938] Loss_D: 0.7455805540084839 Loss_G: 2.078522205352783 D(x): 0.8193010091781616 D(G(z)): 0.3693694472312927/0.15824612975120544\n",
      "[24/25][650/938] Loss_D: 1.140929937362671 Loss_G: 1.61191987991333 D(x): 0.6107239723205566 D(G(z)): 0.3991369307041168/0.23007115721702576\n",
      "[24/25][700/938] Loss_D: 0.7189556360244751 Loss_G: 1.5707801580429077 D(x): 0.7062442302703857 D(G(z)): 0.26271528005599976/0.2387889176607132\n",
      "[24/25][750/938] Loss_D: 0.7553148865699768 Loss_G: 2.230034351348877 D(x): 0.7654781937599182 D(G(z)): 0.32595330476760864/0.12559470534324646\n",
      "[24/25][800/938] Loss_D: 1.0654124021530151 Loss_G: 2.1076245307922363 D(x): 0.8605607748031616 D(G(z)): 0.5338303446769714/0.15810342133045197\n",
      "[24/25][850/938] Loss_D: 0.9183813333511353 Loss_G: 1.2878053188323975 D(x): 0.6293295621871948 D(G(z)): 0.2950051426887512/0.3178809881210327\n",
      "[24/25][900/938] Loss_D: 0.8783046007156372 Loss_G: 1.3778022527694702 D(x): 0.5539724230766296 D(G(z)): 0.19581004977226257/0.28522709012031555\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "niter = 25 \n",
    "\n",
    "for epoch in range( niter ):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizer_D.step()\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"[{epoch}/{niter}][{i}/{len(dataloader)}] Loss_D: {errD.item()} Loss_G: {errG.item()} D(x): {D_x} D(G(z)): {D_G_z1}/{D_G_z2}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu, f\"real_samples.png\", normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(), f\"fake_samples_epoch_{epoch}.png\", normalize=True)\n",
    "    torch.save(netG.state_dict(), f\"netG_epoch_{epoch}.pth\")\n",
    "    torch.save(netD.state_dict(), f\"netD_epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 25\n",
    "latent_dim = 100\n",
    "fixed_noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "fake_images = netG(fixed_noise).cpu().detach().numpy()\n",
    "fake_images = fake_images.reshape(-1, 28, 28)\n",
    "R, C = 5, 5\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(R, C, i+1)\n",
    "    plt.imshow(fake_images[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate GIF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2\n",
    "import glob\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "with imageio.v2.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('fake_samples*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.v2.imread(filename)\n",
    "        writer.append_data(image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "113_Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
